\section{Algoritmi risolutivi}
In questa sezione esamineremo alcuni algoritmi risolutivi proposti per la risoluzione del problema della determinazione della massima bisimulazione su un grafo. Come risulterà evidente nel seguito viene sfruttata ampiamente l'equivalenza tra bisimulazione massima e RCSP dimostrata nella sezione precedente.

\subsection{Minimizzazione di automi a stati finiti}
Esaminiamo innanzitutto l'algoritmo risolutivo per una versione semplificata del problema della determinazione della bisimulazione massima, ovvero la minimizzazione di un'automa a stati finiti, presentato in \cite{hopcroft} nel 1971. Sebbene questa soluzione non sia generale, ha fornito alcuni spunti fondamentali per l'ideazione di algoritmi risolutivi più completi, che verranno presentati nel seguito del lavoro.

\subsubsection{Alcune nozioni fondamentali}
Innanzitutto definiamo il concetto di \emph{automa}, chiaramente centrale nella descrizione del problema:
\begin{definition}
    Consideriamo i seguenti oggetti:
    \begin{itemize}
        \item Un insieme finito $I$ detto \emph{insieme degli ingressi};
        \item Un insieme finito $S$ detto \emph{insieme degli stati}, ad ogni stato è associata un'unica uscita;
        \item Un insieme finito $F \subseteq S$ detto \emph{insieme degli stati finali};
        \item Una funzione $\delta: S \times I E S$ detta \emph{funzione di trasferimento}.
    \end{itemize}
    Chiameremo $A = (S,I,\delta,F)$ \emph{automa}. Useremo la notazione $Out(x)$ per indicare l'output corrispondente allo stato $x$.
\end{definition}
Possiamo rappresentare un'automa con una tabella degli stati, in cui ogni riga rappresenta uno stato, ogni colonna un ingresso, ed ogni cella contiene il nuovo stato del sistema quando, nel momento in cui il sistema si trova nello stato corrispondente alla riga, si inserisce l'ingresso corrispondente alla colonna.In alternativa possiamo utilizzare una rappresentazione grafica, in cui ogni stato è descritto da un cerchio contenente il nome dello stato, e le transizioni tra stati sono descritte da frecce, sulle quali viene specificato l'ingresso che ha innescato la transizione.\\
\begin{example}
    Nella Tabella \ref{fig:tab_automata} è rappresentato un'automa che cambia stato ($A E B$, $B E C$) solamente se l'ingresso è ``1'', e lo stato non è ``$C$''. In qualsiasi altro caso lo stato non cambia.
    \begin{table}[ht]
        \centering
        \begin{tabular}{ c | c c }
            \hline
            & 0 & 1\\
            \hline
            a[0] & a & b \\
            b[0] & b & c \\
            c[1] & c & c \\
            \hline
          \end{tabular}
        \caption{Rappresentazione tabellare di un'automa}
        \label{fig:tab_automata}
    \end{table}
    \label{exa:automata_tab}
\end{example}
\begin{example}
    Nella Figura \ref{fig:automata} è rappresentato lo stesso automa dell'Esempio \ref{exa:automata_tab}.
    \begin{figure}[hb]
        \centering
        \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.5cm,
            scale = 1,transform shape]
            \node[state] (a) {$a[0]$};
            \node[state] (b) [right of=a] {$b[0]$};
            \node[state] (c) [right of=b] {$c[1]$};

            \path (a) edge              node {$1$} (b)
                  (b) edge              node {$1$} (c)
                  (a) edge [loop above]             node {$0$} (a)
                  (b) edge [loop above]             node {$0$} (b)
                  (c) edge [loop above]              node {$0/1$} (c);
        \end{tikzpicture}
        \caption{Rappresentazione grafica di un'automa}
        \label{fig:automata}
    \end{figure}
\end{example}
In alcuni automi è possibile individuare stati che ``si comportano in modo simile''. Informalmente, il sistema si comporta in modo simile quando riceve in input un ingresso qualsiasi, e si trova in uno degli stati presi in esame. Diamo un esempio di questa situazione:
\begin{example}
    \begin{figure}[b]
        \centering
        \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.5cm,
            scale = 1,transform shape]
            \node[state] (a) {$a[0]$};
            \node[state] (b) [right of=a] {$b[1]$};
            \node[state] (c) [right of=b] {$c[1]$};

            \path (a) edge              node {$1$} (b)
                  (b) edge [bend right]             node {$1$} (c)
                  (c) edge [bend right]             node {$1$} (b)
                  (a) edge [loop above]             node {$0$} (a)
                  (b) edge [loop above]             node {$0$} (b)
                  (c) edge [loop above]             node {$0$} (c);
        \end{tikzpicture}
        \caption{Automa contenente stati equivalenti}
        \label{fig:automata_eq}
    \end{figure}
    Consideriamo gli stati dell'automa rappresentato graficamente nella Figura \ref{fig:automata_eq}. Supponiamo di accorpare gli stati $B,C$ in un unico stato, che chiamiamo $B'$. Se ci interessiamo solamente alla sequenza di output ed all'eventuale raggiungimento di uno stato finale, il nuovo automa risulta indistinguibile dal primo.
\end{example}
Proponiamo la definizione formale di equivalenza tra stati che viene utilizzata in \cite{hopcroft}. Nel seguito ne dedurremo un'altra, che consente di stabilire un parallelo con gli argomenti trattati nella Sezione \ref{sec:base}.
\begin{definition}
    \label{def:equivalent_states}
    Sia $I^*$ l'insieme di tutte le sequenze di input di lunghezza finita. Sia $\delta^* : S \times I^*$ la funzione di transizione ``iterata''. Diremo che due stati $x,y$ sono equivalenti (con la notazione ``$x \sim y$'') se e soltanto se valgono congiuntamente le seguenti condizioni:
    \begin{enumerate}
        \item $Out(x) = Out(y)$;
        \item $\forall i^* \in I^*,\,\, \delta^*(x,i^*) \in F \iff \delta^*(y,i^*) \in F$.
    \end{enumerate}
\end{definition}
\accente evidente che individuare g
li stati equivalenti consente di minimizzare il sistema, preservando al tempo stesso i risultati ottenuti. In questo lavoro non illustreremo cosa comporta questa definizione, e perchè è importante che per qualsiasi stringa di lunghezza finita si giunga ad uno stato finale partendo da due stati supposti equivalenti.
\begin{observation}
    La relazione ``$\sim$'' così definita è una relazione di equivalenza sull'insieme degli stati di un'automa.
\end{observation}
La seguente proposizione sarà utile per formulare il problema con la terminologia esposta nella Sezione \ref{sec:rscp}:
\begin{proposition}
    \label{prop:equivalent_states}
    Due stati $x,y$ sono equivalenti nel senso della definizione \ref{def:equivalent_states} se e solo se
    \begin{gather}
        \forall i \in I, \quad \delta(x,i) \sim \delta(y,i)
    \end{gather}
    e $Out(x) = Out(y)$.
\end{proposition}
\begin{proof2}
    Supponiamo che $\exists i \in I : \delta(x,i) \not\sim \delta(y,i)$. Allora, ad esempio:
    \begin{gather*}
        \exists i^* \in I^* : \delta^*(\delta(x,i),i^*) = \delta^*(x,ii^*) \in F, \,\, \delta^*(\delta(y,i),i^*) = \delta^*(y,ii^*) \notin F
    \end{gather*}
    Quindi, per l'esistenza della stringa $ii^*$ si ha $x \not\sim y$. La dimostrazione è speculare se $\exists i^* \in I^* : \delta^*(x,ii^*) \notin F, \delta^*(y,ii^*) \in F$.\\
    Ora supponiamo che valga la (1). E' chiaro che $\forall i^* \in I^*, \forall i \in I$ si ha che
    \begin{gather*}
        \delta^*(x,ii^*) \sim \delta^*(y,ii^*)
    \end{gather*}
    e quindi $x \sim y$.
\end{proof2}
\begin{proposition}
    La relazione ``$\sim$'' con la formulazione dell'Osservazione \ref{prop:equivalent_states} è una bisimulazione sull'insieme degli stati di un'automa, se si considera la relazione binaria $\displaystyle E \,\,\coloneqq \bigcup_{i \in I, x \in S} \{(x,\delta(x,i))\}$.
\end{proposition}
\begin{proof2}
    Supponiamo che $x \sim y$. Sia $x E x'$, cioè $\exists i \in I : x' = \delta(x,i)$. Sia $y' = \delta(y,i) \implies y E y'$. Allora, per l'Osservazione \ref{prop:equivalent_states}, $x' \sim y'$.\\
    Chiaramente lo stesso argomento vale in modo speculare.
\end{proof2}
Osserviamo che nella definizione di ``$E$'' si perde un'informazione importante, cioè il fatto che per $i \in I$ fissato $\delta_i(x) \coloneqq \delta(x,i)$ è una funzione, ovvero l'insieme immagine di ogni $x \in S$ ha cardinalità 1. L'algoritmo di Hopcroft sfrutta questa proprietà nel procedimento che consente di migliorare l'algoritmo banale che verrà discusso nel seguito del lavoro.\\
Possiamo definire la \emph{minimizzazione per stati equivalenti} di un'automa a stati finiti:
\begin{definition}\label{def:minim_eq_states}
    Sia $A = (S,I,\delta,F)$ un'automa. Chiameremo \emph{minimizzazione per stati equivalenti} l'automa $A = (\widetilde{S}, I, \widetilde{\delta}, \widetilde{F})$ dove
    \begin{itemize}
        \item $\widetilde{S} =$ ``l'insieme delle classi di equivalenza di $\sim$ su $S$'';
        \item $\widetilde{\delta} : (\widetilde{S} \times I) E \widetilde{S}, \quad \delta(i, x) = y \implies \widetilde{\delta}(i,\widetilde{x}) = \widetilde{y}$, dove $x,y \in S, i \in I$, e $\widetilde{x}, \widetilde{y} \in \widetilde{S}$ sono rispettivamente le classi di equivalenza di ``$\sim$'' a cui appartengono $x,y$;
        \item $\widetilde{F} =$ ``l'insieme delle classi di equivalenza di $\sim$ su $F$'';
    \end{itemize}
\end{definition}
Prima di concludere l'esposizione delle nozioni preliminari è necessario dimostrare un risultato interessante che lega il problema della minimizzazzione di un'automa a stati finiti con quanto è stato presentato nella sezione \ref{sec:rscp}. A questo scopo dimostriamo i seguenti lemmi, che consentono di dimostrare tale legame in modo agevole:
\begin{lemma}
    \label{lem:part_stab_stesso_blocco}
    Sia $A = (S,I,F,\delta)$ un'automa. Sia $\widehat{S}$ una partizione di $S$ stabile rispetto alle funzioni $\delta_i, \forall i \in I$. Allora $\forall (x,y) \in S \times S$ tali che $[x]_{\widehat{S}} = [y]_{\widehat{S}}$ si ha che $\forall i^* \in I^*$
    \begin{gather*}
        [\delta^*(x,i^*)]_{\widehat{S}} = [\delta^*(y,i^*)]_{\widehat{S}}
    \end{gather*}
\end{lemma}
\begin{proof2}
    Procediamo per induzione su $i^*$. Inizialmente i due stati si trovano nello stesso blocco. Ora supponiamo che dopo l'inserimento dell' $(n-1)$-esimo simbolo di $i^*$ i due stati $x_{n-1}, y_{n-1}$ in cui si trova l'automa appartengano ancora allo stesso blocco. La partizione è stabile rispetto alla funzione $\delta_i$, dove $i$ è l' $n$-esimo simbolo di $i^*$, quindi tutto il blocco $[x_{n-1}]_{\widehat{S}} = [y_{n-1}]_{\widehat{S}}\,\,$ è contenuto all'interno dell'insieme $\delta_i^{-1}([\delta_i(x_{n-1})]_{\widehat{S}}) = \delta_i^{-1}([x_n]_{\widehat{S}})$. Quindi $\delta_i(y_{n-1}) \in \delta_i^{-1}([x_n]_{\widehat{S}})$, e dunque si ha anche $[x_n]_{\widehat{S}} = [y_n]_{\widehat{S}}$.
\end{proof2}
\begin{lemma}
    \label{lem:part_stab_equiv}
    Sia $A = (S,I,F,\delta)$ un'automa. Sia $\widehat{S}$ una partizione di $S$ stabile rispetto alle funzioni $\delta_i, \forall i \in I$. Supponiamo inoltre che per $\widehat{S}$ valga la segente condizione:
    \begin{gather*}
        \forall (x,f) \in S \times F, \quad [x]_{\widetilde{S}} = [f]_{\widetilde{S}} \implies x \in F
    \end{gather*}
    Allora $\forall X \in \widehat{S}, \,\,\forall (x,y) \in X \times X$ si ha $x \sim y$.
\end{lemma}
\begin{proof2}
    Supponiamo per assurdo che in un blocco $X \in \widehat{S}$ esistano due stati $x,y$ non equivalenti. Allora deve esistere una stringa $i^* \in I^*$ tale che, ad esempio, $\delta^*(x,i^*) \in F \land \delta^*(y,i^*) \not\in F$. Ma per il Lemma \ref{lem:part_stab_stesso_blocco} $[\delta^*(x,i^*)]_{\widehat{S}} = [\delta^*(y,i^*)]_{\widehat{S}}$. Chiaramente questo è assurdo per la condizione supposta nell'ipotesi, quindi non possono esistere nello stesso blocco due stati non equivalenti.
\end{proof2}
Possiamo dimostrare il seguente risultato:
\begin{theorem}
    \label{theo:auto_mini_rscp}
    Sia $A$ un'automa a stati finiti. Supponiamo che in tale automa ad uno stato finale ed uno non finale non possa essere assegnato un output identico. La minimizzazione per stati equivalenti proposta nella Definizione \ref{def:minim_eq_states} è l'automa avente per stati i blocchi della partizione più grossolana stabile rispetto alle funzioni $\delta_i : S E S, \forall i \in I$.
\end{theorem}
\begin{proof2}
    Dimostriamo che la minimizzazione proposta nella Definizione \ref{def:minim_eq_states} è una partizione stabile rispetto alle funzioni $\delta_i$. Supponiamo per assurdo che $\exists i \in I : \widetilde{S}$ non è stabile rispetto a $\delta_i$. Quindi $\exists S_1, S_2 \in S :$
    \begin{gather*}
        S_1 \not\subseteq \delta_i^{-1}(S_2) \,\, \land \,\, S_1 \,\cap \,\delta_i^{-1}(S_2) \neq \emptyset
    \end{gather*}
    La prima porzione dell'espressione implica che $\exists s \in S_1 : \delta_i(s) \not\in S_2$. Ma questo è chiaramente in contrasto con l'Osservazione \ref{prop:equivalent_states}, perchè $\forall (u,v) \in S_1 \times S_1$ deve valere $\delta_i(u) \sim \delta_i(v)$, mentre è evidente che, poichè $S_1 \,\cap\, \delta_i^{-1}(S_2) \neq \emptyset$, c'è almeno una coppia che non soddisfa questa condizione.\\
    Ora supponiamo che la partizione $\widetilde{S}$ non sia la più grossolana stabile rispetto alle funzioni $\delta_i : S E S, \forall i \in I$. Ne deve esistere, quindi, una (stabile) più grossolana, che chiamiamo $\widetilde{S}$. Chiaramente si ha $|\widehat{S}| < |\widetilde{S}|$, e quindi devono esistere almeno due blocchi $S_1,S_2 \in \widetilde{S}$ ed un blocco $X \in \widehat{S}$ tali che
    \begin{gather*}
        S_1 \cap X \neq \emptyset \,\,\land\,\, S_2 \cap X \neq \emptyset
    \end{gather*}
    Ma questo è chiaramente in contrasto con il Lemma \ref{lem:part_stab_equiv} per come è stata costruita la partizione $\widetilde{S}$, in quanto esistono coppie di stati $(x,y) \in X \times X$ per cui vale $x \in S_1, y \in S_2$, cioè $x \not\sim y$.
\end{proof2}
Osserviamo che la condizione richiesta nell'ipotesi del Teorema \ref{theo:auto_mini_rscp}, che impone l'assegnazione di un output diverso a stati finali e non finali, è estremamente leggera: dato un'automa qualsiasi è sempre possibile costruirne un altro, che soddisfa tale condizione, in tempo lineare rispetto al numero di stati.

\subsubsection{L'algoritmo naive}
Innanzitutto, con i risultati della sezione precedente, possiamo progettare il seguente algoritmo banale:\\
\begin{algorithm}[H]
    \label{alg:hop_banale}
    \SetAlgoLined
    \KwData{$S,I,\delta$}
    \tcp*[h]{Partizione iniziale contenente un unico blocco}
    \nl$\widetilde{S} \coloneqq \{\{s_1, \dots, s_n\}\}$\;
    \tcc*[h]{Separiamo gli stati non equivalenti in base all'output}\\
    $\widetilde{S} = $ PartizionaPerOutput($\widetilde{S}$)\;\label{alg:hop_banale_partiziona_output}
    \ForEach{$i \in I$} {
        \label{alg:hop_banale_for_ingressi}
        \For{$NS = $ BlocchiNonStabili(S,$\delta_i$)), $NS \neq \emptyset$}{
            \label{alg:hop_banale_for_ns}
            \tcc*[h]{Estraiamo casualmente una coppia di blocchi non stabili}\\
            $(X,Y) = NS[0]$\;
            $X_1 \coloneqq X - \delta_i^{-1}({Y})$\;
            $X_2 \coloneqq X \cap \delta_i^{-1}({Y})$\;
            \tcp*[h]{Aggiorniamo $S$}\\
            $\widetilde{S} = (\widetilde{S} - \{X\}) \cup \{X_1,X_2\}$\;
        }
    }
    \Return{$S$}
    \caption{Procedimento banale per la minimizzazzione}
\end{algorithm}
L'algoritmo termina, perchè la condizione del ciclo diventa sicuramente falsa quando la partizione $S$ è composta da $n$ blocchi, uno per ogni stato, ed ad ogni iterazione un blocco viene diviso in due blocchi distinti e non vuoti. \\
Inoltre la risposta fornita è corretta, perchè l'algoritmo si ferma appena viene trovata una partizione stabile. Poichè ad ogni iterazione del ciclo definito nella Riga \ref{alg:hop_banale_for_ns} il numero di blocchi aumenta di 1, il risultato è la partizione stabile più grossolana rispetto alle funzioni $\delta_i$. Osserviamo inoltre che, se la partizione è stabile rispetto a $\delta_i$ dopo una certa iterazione del ciclo definito nella Riga \ref{alg:hop_banale_for_ingressi}, allora resta stabile fino alla fine dell'esecuzione.\\
Consideriamo la complessità dell'algoritmo:
\begin{itemize}
    \item La Riga \ref{alg:hop_banale_partiziona_output} ha complessità $\Theta(|S|)$;
    \item Il ciclo della Riga \ref{alg:hop_banale_for_ingressi} viene eseguito $\Theta(|I|)$ volte;
    \item La funzione ``BlocchiNonStabili'' ha complessità $O(|S|^2)$;
    \item Il ciclo nella Riga \ref{alg:hop_banale_for_ns} viene eseguito $O(|S|)$ volte;
    \item Il contenuto del ciclo nella Riga \ref{alg:hop_banale_for_ns} ha complessità $O(|S|)$ (con gli opportuni accorgimenti).
\end{itemize}
Quindi la complessità dell'algoritmo è \begin{gather*}
    T_{alg_{\ref{alg:hop_banale}}}(|S|,|I|) = \Theta|I|\left[O(|S|^2 + O(|S|) * O(|S|))\right] = \Theta(I)O(|S|^2)
\end{gather*}
Se consideriamo costante il numero di ingressi: $T_{alg_{\ref{alg:hop_banale}}}(|S|) = O(|S|^2)$.

\subsubsection{L'algoritmo di Hopcroft}
Riportiamo l'algoritmo di Hopcroft, presentato in \cite{hopcroft} nel 1971. Esso migliora la procedura presentata nella sezione precedente, in quanto ha complessità \emph{loglineare}. Forniremo un commento allo pseudocodice, in modo da spiegare il procedimento in modo dettagliato. In seguito analizzeremo formalmente l'algoritmo, proporremo e commenteremo la dimostrazione della correttezza e della complessità.\\
\begin{algorithm}[H]
    \thispagestyle{empty}
    \label{alg:hop}
    \KwData{$S,I,\delta,F$}
    \caption{Algoritmo di Hopcroft}
    \Begin{
        \ForEach{$(s,i) \in S \times I$}{
            $\delta^{-1}(s,i) \coloneqq \{t : \delta(t,i) = s\}$\;\label{alg:hop_deltainverso}
        }
        $B(1) \coloneqq F, B(2) \coloneqq S - F$\;
        \tcc*[h]{$\forall i \in I$ costruiamo l'insieme degli stati in $B(j)$ aventi controimmagine non vuota rispetto  a $\delta_i$}\\
        \ForEach{$j \in \{1,2\}$}{
            \ForEach{$i \in I$}{
                $i(j) = \{s : s \in B(j) \land \delta^{-1}(s,i) \neq \emptyset\}$\;\label{alg:hop_stati_delta_nonvuoto}
            }
        }
        \tcc*[h]{$k$ è il numero di blocchi della partizione. Aumenta dopo le rifiniture}\\
        $k \coloneqq 2$\;
        \tcc*[h]{Per ogni ingresso $i$ creiamo un insieme $L(i)$ contenente l'indice $j$ che minimizza $|i(j)|$}\\
        \ForEach{$i \in I$}{
            \label{alg:hop_splitters}
            \eIf{$|i(1)| \leq |i(2)|$}{
                $L(i) = \{1\}$\;
            }{
                $L(i) = \{2\}$\;
            }
        }
        \While{$\exists i \in I : L(i) \neq \emptyset$}{
            \label{alg:hop_ciclo_infinito}
            Seleziona un $j \in L(i)$. $L(i) = L(i) - \{j\}$\;
            \tcc*[h]{Per ogni blocco $B(m)$ per cui il procedimento ha senso, usiamo $i(j)$ come \emph{splitter}}\\
            \ForEach{$m \leq k : \exists t \in B(m) : \delta(t,i) \in i(j)$}{\label{alg:hop_ciclo_blocchi_preimmagine}
                $B^{'}(m) \coloneqq \{u \in B(m) : \delta(u,i) \in i(j)\}$\;\label{alg:hop_split}
                $B^{''}(m) \coloneqq B(m) - B^{'}(m)$\;
                $B(m) = B^{'}(m)$, $B(k+1) \coloneqq B^{''}(m)$\;
                \ForEach{$a \in I$}{\label{alg:hop_aggiorna_dopo_split}
                    $a(m) = \{s : s \in B(m) \land \delta^{-1}(s,a) \neq \emptyset\}$\;
                    $a(k+1) \coloneqq \{s : s \in B(k+1) \land \delta^{-1}(s,a) \neq \emptyset\}$\;
                    \eIf{$|a(m)| \leq |a(k+1)|$}{
                        $L(a) = L(a) \cup \{m\}$\;
                    }{
                        $L(a) = L(a) \cup \{k+1\}$\;
                    }
                }
                $k = k + 1$\;
            }
        }
    }
    \thispagestyle{empty}
\end{algorithm}
Nella Riga \ref{alg:hop_deltainverso} definiamo $\delta^{-1}$, che consente l'accesso in tempo costante all'insieme degli stati che conducono ad un determinato stato conseguentemente ad un determinato ingresso.\\
Inizialmente la partizione contiene due blocchi: gli stati finali e quelli non finali. Di conseguenza le rifiniture successive della partizione manterranno sempre separati stati finali e non finali.\\
Per evitare accessi inutili (che incrementerebbero i termini costanti dell'algoritmo) nella Riga \ref{alg:hop_stati_delta_nonvuoto} definiamo $i(j)$ per $i \in I, j \in \{1,2\}$ come l'insieme degli stati $s$ nel blocco $B(j)$ per cui esiste qualche stato $t$ tale che $\delta(t,i) = s$.\\
All'interno del ciclo della Riga \ref{alg:hop_splitters} definiamo gli insiemi $L(i)\,\, \forall i \in I$, che contengono gli indici dei blocchi che verranno usati come \emph{splitter} nel seguito del procedimento. Osserviamo che all'interno di questo insieme vengono inseriti gli indici dei blocchi per cui la cardinalità di ``$i(\cdot)$'' è minima. Come dimosteremo nel seguito, questa tecnica

In \cite{paigetarjan} si dimostra che, limitatamente al caso particolare a cui si applica l'algoritmo di Hopcroft, è possibile scegliere gli \emph{splitter} in modo vantaggioso, allo scopo di ridurre il carico di lavoro e dunque la complessità. Riportiamo e commentiamo la dimostrazione:
\begin{proposition}
    Sia $S$ un insieme finito. Sia $f : S E S$ una funzione (cioè $\forall s \in S\,\, |f({s})| = 1$). Sia $\widetilde{S}$ una partizione di $S$. Sia $Q$ l'unione di alcuni blocchi di $\widetilde{S}$, e sia $B$ un blocco di $\widetilde{S}$, con $B \subseteq Q$. Supponiamo $\widetilde{S}$ stabile rispetto a $Q$. Allora
    \begin{center}
        Split$(B,\widetilde{S})$ = Split$(Q - B, $Split$(B,\widetilde{S}))$
    \end{center}
\end{proposition}
\begin{proof2}
    Vogliamo dimostrare che $Q - B$ non è uno \emph{splitter} di Split$(B,\widetilde{S})$, cioè che $\forall S_x \in $ Split$(B,\widetilde{S})$ si ha
    \begin{gather*}
        S_x \subseteq f^{-1}(Q - B) \,\,\lor\,\, S_s \cap f^{-1}(Q-B) = \emptyset
    \end{gather*}
    Ricordando che $\widetilde{S}$ è stabile rispetto a $Q$, $\widehat{S} \coloneqq$ Split$(B,\widetilde{S})$ è stabile rispetto a $Q$ e a $B$, per l'Osservazione \ref{prop:split_eredita}. Di conseguenza per ogni blocco $S_x \in \widehat{S}$ si ha
    \begin{gather*}
        S_x \subseteq f^{-1}(B) \lor S_x \cap f^{-1}(B) = \emptyset\\
        \land\\
        S_x \subseteq f^{-1}(Q) \lor S_x \cap f^{-1}(Q) = \emptyset
    \end{gather*}
    Se $S_x \subseteq f^{-1}(B)$ chiaramente $S_x \,\,\cap \subseteq f^{-1}(Q-B) = \emptyset$. Se $S_x \cap f^{-1}(Q) = \emptyset$, allora $S_x \cap f^{-1}(Q-B) = \emptyset$. Se $S_x \cap f^{-1}(B) = \emptyset \land S_x \subseteq f^{-1}(Q)$ bisogna avere $S_x \subseteq f^{-1}(Q-B)$.\\
    Osserviamo che queste deduzioni valgono solamente se $f$ è una funzione.
\end{proof2}

Dal punto di vista computazionale è chiaramente più conveniente usare come \emph{splitter} l'insieme tra $B$ e $Q - B$ avente cardinalità minore. La strategia di Hopcroft (``\emph{process the smaller half}'', come viene sintetizzata in \cite{paigetarjan}) consiste infatti nella selezione degli \emph{splitter} secondo il criterio della cardinalità, a differenza di quanto avviene nell'Algoritmo \ref{alg:hop_banale}.\\
Dalla Riga \ref{alg:hop_split} alla Riga \ref{alg:hop_aggiorna_dopo_split} viene operato lo ``Split''. Nel ciclo della Riga \ref{alg:hop_aggiorna_dopo_split} vengono aggiornate le strutture dati relative ai nuovi blocchi creati. Osserviamo che ad ogni iterazione del ciclo della Riga \ref{alg:hop_aggiorna_dopo_split} vengono creati due nuovi blocchi, anche se il numero di blocchi aumenta soltanto di 1, perchè viene anche ``smembrato'' un blocco già esistente. Di questi due blocchi se ne sceglie uno da usare come \emph{splitter}, seguendo il criterio illustrato sopra.\\
Queste operazioni vengono ripetute finchè in $L(i)$ per qualche $i$ resta un blocco da usare come \emph{splitter}.
\\\\
Analizziamo la correttezza dell'algoritmo. Trascurando la parte iniziale, ad ogni iterazione del ciclo della Riga \ref{alg:hop_ciclo_infinito} viene prima rimosso un elemento in $L(i)$, e solamente in seguito ad una rifinitura della partizione ne viene inserito un altro. Poichè non è possibile rifinire all'infinito, l'algoritmo termina.\\
Il seguente Teorema dimostra la validità del risultato:
\begin{theorem}
    \label{teo:hop_corretto}
    Sia $A = (S,I,\delta,F)$ un'automa. Sia $\widetilde{S}$ la partizione risultante dall'applicazione dell'Algoritmo \ref{alg:hop}. Siano $x,y \in S$. Allora
    \begin{gather*}
        x \sim y \iff [x]_{\widetilde{S}} = [y]_{\widetilde{S}}
    \end{gather*}
\end{theorem}
\begin{proof2}
    Dimostriamo innanzitutto che $[x]_{\widetilde{S}} \neq [y]_{\widetilde{S}} \implies x \not\sim y$. Procediamo per induzione:
    \begin{itemize}
        \item La relazione vale prima del ciclo \ref{alg:hop_ciclo_infinito}, infatti stati finali e non finali non possono essere equivalenti;
        \item Supponiamo che sia vero prima di una certa iterazione. Supponiamo che gli stati $x,y$ finiscano in partizioni diverse nelle Righe tra \ref{alg:hop_split} e \ref{alg:hop_aggiorna_dopo_split}. Questo significa che $[\delta(x,i)]_{\widetilde{S}_n} \neq [\delta(y,i)]_{\widetilde{S}_n}$, dove $\widetilde{S}_n$ è la partizione costruita dall'algoritmo fino all'iterazione considerata. Ma allora, per l'ipotesi induttiva, $\delta(x,i) \not\sim \delta(y,i)$, e quindi $x,y$ non sono equivalenti (per l'Osservazione \ref{prop:equivalent_states}).
    \end{itemize}
    Questo ragionamento è valido perchè il fatto che due stati si trovino in partizioni differenti dopo una certa iterazione implica che si troveranno in partizioni differenti anche al termine del procedimento.\\
    Dimostriamo ora che $[x]_{\widetilde{S}} = [y]_{\widetilde{S}} \implies x \sim y$. Supponiamo per assurdo che per due stati $x,y$ si abbia $[x]_{\widetilde{S}} = [y]_{\widetilde{S}} \land x \not\sim y$. Allora, ad esempio, $\exists i^* \in I^* : \delta^*(x,i^*) \in F, \delta^*(y,i^*) \not\in F$. Ma poichè inizialmente poniamo stati inziali e finali in partizioni diverse, deve valere chiaramente $[\delta^*(x,i^*)]_{\widetilde{S}} \neq [\delta^*(y,i^*)]_{\widetilde{S}}$, e quindi procedendo a ritroso sui simboli di $i^*$ si ha
    \begin{gather*}
        [\delta^*(x,i^*_n)]_{\widetilde{S}} \neq [\delta^*(y,i^*)_n]_{\widetilde{S}} \implies [\delta^*(x,i^*_{n-1})]_{\widetilde{S}} \neq [\delta^*(y,i^*)_{n-1}]_{\widetilde{S}}
    \end{gather*}
    Per cui si ha chiaramente $[x]_{\widetilde{S}} = [\delta^*(x,i^*_0)]_{\widetilde{S}} \neq [\delta^*(y,i^*)_0]_{\widetilde{S}} = [y]_{\widetilde{S}}$.
\end{proof2}

Discutiamo ora la complessità dell'algoritmo. In questo paragrafo non tratteremo i dettagli dell'implementazione, facilmente reperibili in \cite{hopcroft}, e ci concentreremo unicamente sul contributo del procedimento.\\
Per costruire $\delta^{-1}$ è sufficiente valutare una sola volta ogni stato per ogni ingresso, quindi l'operazione è $\Theta(|S||I|)$. La costruzione delle due partizioni iniziali è $\Theta(|S|)$. La costruzione di $L(i) \,\,\forall i \in I$ è chiaramente $\Theta(|I|)$. Di conseguenza la complessità delle istruzioni precedenti alla Riga \ref{alg:hop_ciclo_infinito} è $\Theta(|S||I|)$.\\
Prima di procedere, dimostriamo il seguente risultato:
\begin{proposition}
    \label{obs:log}
    Sia
    \begin{gather*}
        f_a(x) \coloneqq a\log_2(a) - (a-x)\log_2(a-x) - x\log_2(x) \qquad a > 0,\,\, 0 < x < a
    \end{gather*}
    Tale funzione ha massimo in $\frac{a}{2}$, con $f_a(\frac{a}{2}) = a$, ed è strettamente positiva sul dominio considerato.
\end{proposition}
\begin{proof2}
    L'osservazione si dimostra con un semplice studio di funzione.
\end{proof2}
Il seguente risultato fornisce un \emph{upper-bound} per il contributo al tempo di esecuzione di tutte le iterazioni del ciclo in cui si seleziona l'ingresso $i$ considerato sopra, dalla $n$-esima fino alla terminazione dell'algoritmo:
\begin{theorem}
    Sia $i$ un ingresso. Consideriamo l'iterazione $n$-esima del ciclo della Riga \ref{alg:hop_ciclo_infinito} per un $n$ qualsiasi per cui si abbia la seguente configurazione:
    \begin{gather*}
        \widetilde{S}_n = \{S_1,\dots,S_m\}, \quad L(i) = \{i_1,\dots,i_r\}, \quad  \{i_{r+1},\dots,i_m\} = \{1,\dots,m\} - L(i)
    \end{gather*}
    Allora il contributo alla complessità dato da tutte le iterazioni in cui si seleziona l'ingresso $i$, dalla $n$-esima fino alla terminazione dell'algoritmo, è maggiorato dalla seguente espressione:
    \begin{gather*}
        T_i^n = k\left(\sum_{j = 1}^r |i(i_j)|\log_2|i(i_j)| + \sum_{j = r+1}^m |i(i_j)|\log_2\frac{|i(i_j)|}{2}\right)
    \end{gather*}
\end{theorem}
\begin{proof2}
    Procediamo per induzione ``al contrario''. Chiaramente la maggiorazione è valida alla terminazione dell'algoritmo, ovvero dopo l'ultima iterazione. Ora supponiamo che la maggiorazione sia valida dopo l'iterazione $k$-esima (con $k > n$), e dimostriamo che questo implica la validità della stessa prima dell'iterazione $k$-esima, ovvero al termine dell'iterazione $(k-1)$-esima. In altre parole è necessario dimostrare che la somma tra $T_i^{k-1}$ ed il tempo impiegato per l'esecuzione dell'iterazione $k$-esima è minore o uguale di $T_i^k$. La seguente osservazione consente di quantificare il contributo di una singola iterazione:
    \begin{observation*}
        Se all'inizio dell'iterazione viene selezionato l'indice $x \in L(i)$, la complessità dell'iterazione è $O(|i(x)|)$, cioè il tempo impiegato durante l'esecuzione è maggiorato da $k|i(x)|$, dove $k$ è una costante di propozionalità. Possiamo trarre questa conclusione perchè il ciclo della Riga \ref{alg:hop_ciclo_blocchi_preimmagine} è in realtà un ciclo sugli stati $s\in i(x)$, che per ognuno estrae (in tempo costante) $\delta^{-1}(s,i)$.
    \end{observation*}
    All'inizio di ogni iterazione viene estratto un ingresso $a$. Si presentano due casi:
    \begin{itemize}
        \item $a \neq i$: poichè $T_i^k$ prende in considerazione solamente il tempo impiegato dalle iterazioni in cui viene selezionato $i$, questa iterazione è esclusa dalla stima. Ciononostante l'iterazione può modificare i blocchi della partizione $\widetilde{S}_k$, e dobbiamo quindi verificare che $T_i^{k-1} \leq T_i^k$:
        \begin{itemize}
            \item Se viene modificato un blocco $B(x)$ con $x \in L(i)$, nella stima dobbiamo sostituire un elemento maggiorabile con $b \log_2 b$ con un elemento del tipo $c \log_2 c + (b-c) \log_2 (b-c)$. Per l'osservazione \ref{obs:log} si ha $T_i^{k-1} < T_i^k$;
            \item Se invece $x \not\in L(i)$, in $T_i^{k-1}$ dobbiamo sostituire un elemento del tipo $b \log_2 \frac{b}{2}$ con un elemento del tipo $c \log_2 c + (b-c) \log_2 \frac{b-c}{2}$ (supponendo che $c \leq b - c$, in caso contrario la dimostrazione è simile). Infatti il blocco avente cardinalità $b-c$ fa parte, alla fine dell'iterazione, dell'insieme dei blocchi il cui indice non appartiene ad $L(i)$, ed al termine del ciclo della Riga \ref{alg:hop_aggiorna_dopo_split} si ha $c \in L(i)$. Chiaramente si ha $c \leq \frac{b}{2}$, e quindi:
            \begin{align*}
                c \log_2 c + (b-c) \log_2 \frac{b-c}{2} &\leq c \log_2 \frac{b}{2} + (b-c) \log_2 \frac{b}{2}\\
                &= (c + b - c) \log_2 \frac{b}{2}\\
                &= b \log_2 \frac{b}{2}.
            \end{align*}
        \end{itemize}
        \item $a = i$: come abbiamo osservato sopra, il tempo impiegato all'interno dell'iterazione è $O(|i(x)|)$, dove $x$ è l'indice estratto da $L(i)$. Poichè si ha:
        \begin{align*}
            T_i^{k-1} + k|i(x)| = k \Bigg( |i(x)| + |i(x)|\log_2 \frac{|i(x)|}{2} &+ \sum_{\substack{j = 1\\i_j \neq x}}^r |i(i_j)|\log_2|i(i_j)| \\
            &+ \sum_{j = r+1}^m |i(i_j)|\log_2\frac{|i(i_j)|}{2}\Bigg)
         \end{align*}
        dove consideriamo il tempo impiegato per l'iterazione, ed il fatto che l'indice $x$, al termine dell'iterazione, fa parte dell'insieme dei blocchi il cui indice non appartiene ad $L(i)$. Poichè vale:
        \begin{align*}
            |i(x)| + |i(x)|\log_2 \frac{|i(x)|}{2} &= |i(x)| + |i(x)|\log_2 |i(x)| - |i(x)|\log_2 2\\
            &= |i(x)| + |i(x)|\log_2 |i(x)| - |i(x)|\\
            &= |i(x)|\log_2 |i(x)|
        \end{align*}
        si deduce facilmente la seguente relazione:
        \begin{gather*}
            T_i^{k-1} + k|i(x)| = T_i^k.
        \end{gather*}
    \end{itemize}
    \vspace*{-0.8cm}
\end{proof2}
Prima della prima iterazione del ciclo della Riga \ref{alg:hop_ciclo_infinito}, per un $i \in I$ fissato, (supponendo $|S-F| \geq |F|$) si ha
\begin{gather*}
    T_i = k\left(|S-F|\log_2|S-F| + |F|\log_2\frac{|F|}{2}\right)
\end{gather*}
che, per l'Osservazione \ref{obs:log}, si maggiora con $k |S|\log_2|S|$. Allora la complessità dell'algoritmo è data dalla somma della complessità delle righe precedenti alla \ref{alg:hop_ciclo_infinito}, ovvero $\Theta(|I||S|)$, e $|I| * O(|S|\log|S|)$, cioè $|I||S|\log|S|$. Considerando costante la cardinalità di $I$:
\begin{gather*}
    T_{alg} = O(|S|\log|S|).
\end{gather*}

\newpage
\subsection{L'algoritmo di Paige-Tarjan}
In questa sezione, dopo aver trattato il caso specifico risolto dall'algoritmo di Hopcroft, discutiamo l'algoritmo di Paige-Tarjan presentato in \cite{paigetarjan}. Il procedimento ha complessità $O(|E|\log |V|)$ (dati un insieme $V$ ed una relazione $E$), ed è il primo a risolvere il caso generale migliorando la complessità $O(|E||V|)$ manifestata dall'algoritmo di Kanellakis e Smolka, che non adotta particolari accorgimenti per la selezione dei blocchi da usare come \emph{splitter}.

\subsubsection{L'algoritmo}
Proponiamo e commentiamo innanzitutto lo pseudocodice:\\
\begin{algorithm}[H]
    \label{alg:pt}
    \KwData{$V,E,\widetilde{V}$}
    \caption{Algoritmo di Paige-Tarjan}
    \Begin{
        $Q \coloneqq \widetilde{V}, X \coloneqq \{V\}$\;
        \tcp*[h]{I blocchi di $Q$ vengono rifiniti rispetto a $E^{-1}(V)$}\\
        $Q = \splitfunc(V,Q)$\; \label{alg:pta_rifi_iniziale}
        \While{$Q \neq X$}{
            Scelgo in modo casuale un blocco $S \in X \mid B \not\in Q$\;
            Scelgo un blocco $B \in X \mid B \subset S, |B| \leq |S|/2$\;
            $X = (X - \{S\}) \cup \{B, S-B\}$\;
            $Q = \splitfunc(S-B, \splitfunc(B,Q))$\;\label{alg:pt_doublesplit}
        }
        \Return{$Q$}\;
    }
\end{algorithm}

L'algoritmo definisce inizialmente due partizioni $Q,X$: la prima coincide con la partizione iniziale $\widetilde{V}$, mentre la seconda è la partizione banale formata da un unico blocco contenente tutto $V$.

Alla Riga \ref{alg:pta_rifi_iniziale} viene imposta l'invariante che verrà poi mantenuta per tutta l'esecuzione dell'algoritmo: ogni blocco di $Q$ viene diviso in due blocchi (di cui uno eventualmente vuoto): nel primo vengono inseriti i nodi che sono sorgente di almeno un arco; nel secondo vengono inseriti i pozzi (cioè gli elementi dell'insieme $B - E^{-1}(V)$, dove $B$ è un blocco di $Q$). Dopo questa operazione la partizione $Q$ è stabile rispetto a $V$.

Il corpo del procedimento è formato da un ciclo che ripete le seguenti istruzioni finchè le partizioni $Q,X$ non coincidono:
\begin{enumerate}
    \item Viene scelto casualmente un blocco $S \in X$ non appartenente a $Q$;
    \item Viene scelto casualmente un blocco $B \in Q \mid B \subset S$, e la cui cardinalità sia inferiore a $|S|/2$;
    \item In $X$ il blocco $S$ viene sostituito da $\{B, S-B\}$;
    \item Si sostituisce la partizione $Q$ con \splitfunc$(S-B,$ \splitfunc$(B,Q))$.
\end{enumerate}

Quando $Q$ e $X$ coincidono, viene restituita la partizione $Q$.

\begin{observation}
    \accente sempre possibile trovare un blocco in $X$ per portare a termine il passaggio 1. Quando non è possibile l'algoritmo termina, perchè $Q = X$.
\end{observation}

\subsubsection{Analisi}
Dimostriamo innanzitutto la validità della seguente invariante:
\begin{lemma}
    \label{lem:pt_qx}
    L'algoritmo ristabilisce le seguenti relazioni invarianti dopo ogni iterazione del ciclo principale:
    \begin{enumerate}
        \item $Q$ è stabile rispetto ad ogni blocco di $X$;
        \item $Q$ rifinisce $X$;
        \item $\rscp(\widetilde{V},E)$ rifinisce $Q$.
    \end{enumerate}
\end{lemma}
\begin{proof2}
    In entrambi i casi procediamo per induzione sulle iterazioni:
    \begin{itemize}
        \item[1/2.] Prima della prima iterazione $Q$ è stabile rispetto a $X$ per l'operazione eseguita alla Riga \ref{alg:pta_rifi_iniziale}. Inoltre $Q$ rifinisce $X$ perchè quest'ultima è la partizione banale di $V$.

        Ora supponiamo che l'invariante sia valida prima di un'iterazione qualsiasi: durante l'iterazione successiva in $X$ viene sostituito il blocco $S$ con i blocchi $S-B, B$, mentre $Q$ viene modificato da sue applicazioni successive della funzione Split. E' evidente che $Q$ deve essere stabile rispetto a $S-B, B$, ed anche a tutti gli altri blocchi di $X$ per l'ipotesi induttiva. Inoltre, sempre per l'ipotesi induttiva, devono esistere (prima della modifica) dei blocchi $C_1, \dots, C_n \mid \bigcup_{i=1}^n C_i = S$. Dopo la modifica di $Q$ l'unione dei nuovi blocchi generati dall'applicazione dei due Split ai blocchi $\{C_1, \dots, C_n\} - B$ è $S-B$, mentre l'unione dei due nuovi blocchi (di cui uno eventualmente vuoto) generati dalla divisione di $B$ è chiaramente $B$. Per cui $Q$ rifinisce ancora $X$.
        \item[3.] La proprietà è vera banalmente prima della prima iterazione.

        Supponiamo che l'invariante sia valida prima dell'iterazione $i$-esima: durante l'iterazione successiva la
        partizione $Q$ viene rifinita utilizzando come \emph{splitter} i blocchi $B, S-B$. Poichè $Q$ rifinisce $X$, e $S$ è un blocco di $X$, $S$ è l'unione di alcuni blocchi di $Q$. Inoltre, essendo $B$ un blocco di $Q$ (con $B \subset S$), anche $S-B$ è unione di alcuni blocchi di $Q$. Allora, poichè per l'ipotesi induttiva $\rscp(\widetilde{V},E)$ rifinisce $Q$:
        \begin{gather*}
            \rscp(\widetilde{V},E) = \splitfunc(S-B, \splitfunc(B, \rscp(\widetilde{V},E)))
        \end{gather*}
        Quindi, per il Teorema \ref{theo:split_properties} (monotonia di \splitfunc), $\rscp(\widetilde{V},E)$ rifinisce la nuova partizione $Q' = \splitfunc(S-B, \splitfunc(B, Q))$.
    \end{itemize}
    \vspace*{-0.75cm}
\end{proof2}
Possiamo ora dedurre in modo diretto il seguente risultato:
\begin{corollary}
    L'Algoritmo \ref{alg:pt} termina.
\end{corollary}
\begin{proof2}
    Ad ogni iterazione il numero di blocchi in $X$ aumenta di una unità. Quindi in $O(|V|)$ iterazioni si avrà $X = \{\{v\} \mid v \in V\}$ (se l'algoritmo non termina prima). Ma per il Lemma \ref{lem:pt_qx} $Q$ rifinisce $X$ prima di ogni iterazione, quindi deve essere necessariamente $Q = \{\{v\} \mid v \in V\} = X$.
\end{proof2}
Dimostriamo ora che l'algoritmo è corretto:
\begin{proposition}
    Al termine dell'Algoritmo \ref{alg:pt} $Q = \rscp(E,\widetilde{V})$.
\end{proposition}
\begin{proof2}
    Per il Lemma \ref{lem:pt_qx}, la partizione $Q$ è sempre stabile rispetto ad ogni blocco di $X$. Poichè la condizione di terminazione è $Q=X$, $Q$ è stabile. Ma allora, sempre per il Lemma \ref{lem:pt_qx} (punto 3), $Q = \rscp(\widetilde{V},E)$.
\end{proof2}

\subsubsection{Complessità}
Riportiamo e commentiamo il Lemma 3 proposto in \cite{paigetarjan}:
\begin{lemma}
    \label{lem:pt_lemma3}
    Valgono le seguenti uguaglianze insiemistiche per il ``doppio \splitfunc'' della Riga \ref{alg:pt_doublesplit}:
    \begin{enumerate}
        \item $\splitfunc(B,Q)$ divide i blocchi $D \in Q$ in due blocchi $D_1 = D \cap E^{-1}(B), D_2 = D - D_1$. $D_1, D_2$ sono entrambi non vuoti se e solo se $D \cap E^{-1}(B) \neq \emptyset \land D - E^{-1}(B) \neq \emptyset$;
        \item $\splitfunc(S-B,\splitfunc(B,Q))$ divide i blocchi $D_1$ di $\splitfunc(B,Q)$ in due blocchi $D_{11} = D_1 \cap E^{-1}(S-B), D_{12} = D_1 - D_{11}$. $D_{11}, D_{12}$ sono entrambi non vuoti se e solo se $D_1 \cap E^{-1}(S-B) \neq \emptyset \land D_1 - E^{-1}(S-B) \neq \emptyset$;
        \item $\splitfunc(S-B,\splitfunc(B,Q))$ non modifica i blocchi di tipo $D_2$;
        \item $D_{12} = D_1 \cap (E^{-1}(B) - E^{-1}(S-B))$.
    \end{enumerate}
\end{lemma}
\begin{figure}[t]
    \centering

    \begin{venndiagram3sets}[labelA=$D_1$, labelB=$E^{-1}(B)$, labelC=$E^{-1}(S-B)$, showframe=false, labelOnlyAB=$D_{12}$]
        \fillACapBNotC
    \end{venndiagram3sets}

    \caption{Punto 4. del Lemma \ref{lem:pt_lemma3}}
    \label{fig:pt_lemma_insiemi}
\end{figure}
\begin{proof2}
    La dimostrazione è banale per i primi due punti (è una conseguenza diretta della Definizione \ref{def:funz_split}).
    \begin{enumerate}
        \item[3.] Supponiamo che un blocco $D \in Q$ sia stato diviso in due blocchi non vuoti $D_1, D_2$ da $\splitfunc(B,Q)$. Allora, per il punto 1, $D \cap E^{-1}(B) \neq \emptyset$. Quindi, per la stabilità di $Q$ rispetto a $(E,S)$, deve valere $D \subseteq E^{-1}(S)$. Allora $D_2 \subset D \subseteq E^{-1}(S)$. Per definizione $D_2$ e $E^{-1}(B)$ sono disgiunti, quindi $D_2 \subseteq E^{-1}(S-B)$;
        \item[4.] Evidente nella Figura \ref{fig:pt_lemma_insiemi}.
    \end{enumerate}
    \vspace*{-0.75cm}
\end{proof2}

\accente chiaro che ad ogni iterazione l'Algoritmo \ref{alg:pt} divide alcuni blocchi di $Q$ in $D_{11}, D_{12}, D_2$.

\begin{observation}
    Con opportuni accorgimenti è possibile implementare una funzione che divide ogni blocco di $Q$ in $D_{11}, D_{12}, D_2$ avente complessità $O(|B| + \sum_{y \in B} |E^{-1}(\{y\})|)$, dove $B$ è il blocco usato come \emph{splitter}. I dettagli sono discussi in \cite{paigetarjan}.
\end{observation}

Da questa implementazione possiamo dedurre il seguente upper-bound sulla complessità dell'Algoritmo di Paige-Tarjan:

\begin{theorem}
    La complessità dell'Algoritmo \ref{alg:pt} è $O(|E|\log |V|)$.
\end{theorem}
\begin{proof2}
    Chiaramente la complessità è:
    \begin{align*}
        \displaystyle &\sum_{\substack{B \mid B \text{ è usato}\\\text{come \emph{splitter}}}} O(|B| + \sum_{y \in B} |E^{-1}(\{y\})|)\\
        &= \sum_{\substack{B \mid B \text{ è usato}\\\text{come \emph{splitter}}}} O(|B|) + \sum_{\substack{B \mid B \text{ è usato}\\\text{come \emph{splitter}}}} \sum_{y \in B} |E^{-1}(\{y\})|)
    \end{align*}

    \begin{observation}
        Ogni elemento $x \in S$ può apparire al più $\log_2 (|V| + 1)$ volte in un blocco usato come \emph{splitter}, visto che ad ogni nuova iterazione un eventuale blocco contentente questo stesso $x$ ha cardinalità dimezzata.
    \end{observation}

    Allora:
    \begin{align*}
        \sum_{\substack{B \mid B \text{ è usato}\\\text{come \emph{splitter}}}} \sum_{y \in B} |E^{-1}(\{y\})|) &\leq \log_2 (|V|+1) \sum_{x \in V} |E^{-1}(\{x\})|\\
        &= \log_2 (|V|+1) |E|\\
        &= O(|E|\log |V|)
    \end{align*}

    Inoltre:
    \begin{align*}
        \sum_{\substack{B \mid B \text{ è usato}\\\text{come \emph{splitter}}}} O(|B|) &\leq \sum_{x \in V} \log_{2} (|V|+1)\\
        &= |V|\log_{2} (|V|+1)\\
        &= O(|V|\log |V|)
    \end{align*}

    \accente lecito considerare $|V| = O(|E|)$, per cui la complessità è $O(E\log |V|)$.
\end{proof2}

\newpage
\subsection{L'algoritmo di Dovier-Piazza-Policriti}
In questa sezione trattiamo l'Algoritmo risolutivo presentato in \cite{dovier}, che chiameremo anche ``FBA''. Non c'è un miglioramento generale per quanto riguarda la complessità, che resta $O(|E| \log |N|)$ come per l'Algoritmo \ref{alg:pt}; tuttavia è possibile integrare alcune strategie euristiche che consentono di ridurre drasticamente questo upper-bound, avvicinandolo in aluni casi ad una computazione lineare.

Prima di presentare e commentare lo pseudocodice, proponiamo delle definizioni fondamentali, insieme ad alcune osservazioni. In seguito proporremo la dimostrazione della correttezza e della complessità dell'Algoritmo FBA nel caso peggiore, e discuteremo alcune delle euristiche menzionate sopra.

\subsubsection{Nozioni preliminari}
Iniziamo con la seguente definizione fondamentale:
\begin{definition}
    \label{def:grafo_restr}
    Sia $G = (V,E)$ un grafo diretto. Sia $n \in V$. Il grafo $\restr{G}{n}$, cioè il \emph{sottografo di $G$ raggiungibile da $n$}, è definito come segue:
    \begin{gather*}
        \restr{G}{n} = \left(\restr{N}{n}, E \cap \left(\restr{N}{n} \times \restr{N}{n}\right)\right)
    \end{gather*}
    dove $\restr{N}{n}$ è il sottoinsieme di $V$ dei nodi raggiungibili da $n$.
\end{definition}
Nel seguito useremo la notazione ``$[n]$'' per intendere il blocco di $V^{\superscc}$ in cui viene inserito il vertice $n$, coerentemente con quando asserito nella Definizione \ref{def:scc_partition}.

Sulla base dell'ultima definizione possiamo distinguere una porzione del grafo per una caratteristica importante:
\begin{definition}
    La parte \emph{ben fondata} (\emph{well founded} in inglese) di un grafo diretto $G = (V,E)$ è:
    \begin{gather*}
        WF(G) \coloneqq \left\{n \in V \mid \restr{G}{n} \text{ è aciclico}\right\}
    \end{gather*}
\end{definition}
\begin{observation}
    Condizione necessaria affinchè un vertice $n$ possa appartenere a $WF(G)$ è che $|[n]| = 1$.
\end{observation}
\begin{proof2}
    Supponiamo che $n \in WF(G)$. Questo vuol dire che da tutti i nodi raggiungibili da $n$ non si può ritornare in $n$ (perchè non si hanno cicli). Ma allora non ci può essere un altro nodo nella SCC di $n$.
\end{proof2}

Introduciamo la funzione ``\rankfunc'', che ricopre un ruolo primario nell'Algoritmo BFA:
\begin{definition}
    Sia $G = (V,E)$ un grafo diretto. Per comodità definiamo le seguenti funzioni:
    \begin{align*}
        &\mathcal{N}_{\scalebox{0.5}{WF}}(n) = \{m \in WF(G) \mid [n] \sccto [m]\}\\
        &\mathcal{N}_{\scalebox{0.5}{NWF}}(n) = \{m \in V - WF(G) \mid [n] \sccto [m]\}
    \end{align*}
    Esse associano ad un nodo $n \in V$ l'insieme delle SCC raggiungibili da $[n]$, rispettivamente contenute in $WF(G), V - WF(G)$.
    La funzione $\rankfunc: V \to \mathbb{N} \cup \{0, -\infty\}$ è definita come segue:
    \begin{gather*}
        \rankfunc(n) = \begin{cases}
            0 &\text{se $n$ è un pozzo di $G$}\\
            -\infty &\text{se $[n]$ è un pozzo di $G^{\scalebox{.5}{SCC}}$, $n$ non è un pozzo di $G$}\\
            \max(\{1 + \rankfunc(m) \mid m \in \mathcal{N_{\scalebox{0.5}{WF}}}(n)\}\\
            \,\,\,\,\,\,\,\,\,\,\,\,\,\, \cup \,\, \{\rankfunc(m) \mid m \in \mathcal{N_{\scalebox{0.5}{NWF}}}(n)\}) &\text{altrimenti}
        \end{cases}
    \end{gather*}
\end{definition}
\begin{observation}
    Sia $n \in V$, e supponiamo $\rankfunc(n) = k$. Allora:
    \begin{gather*}
        \rankfunc(m) = k \,\,\forall m \in [n]
    \end{gather*}
\end{observation}
\begin{proof2}
    Se $k = 0$ $n$ è una pozzo, per cui la sua SCC ha cardinalità 1, quindi la tesi vale banalmente. Se $k = -\infty$, nessuno dei nodi in $[n]$ può essere un pozzo di $G$, per cui hanno tutti rango $-\infty$. Quest'ultima argomentazione si può applicare anche al caso in cui $k > 0$, osservando che la definizione della funzione \rankfunc \, non distingue tra nodi appartenenti alla stessa SCC.
\end{proof2}
Se un grafo è \emph{ben fondato}, o se restringiamo il dominio ad un sottografo \emph{ben fondato}, possiamo dare una formulazione alternativa della funzione rank:
\begin{gather*}
        \rankfunc(n) = \begin{cases}
            0 &\text{se $n$ è un pozzo di $G$}\\
            1 + \max\{\rankfunc(m) \mid n E m\} &\text{altrimenti}
        \end{cases}
\end{gather*}
Questa forma sarà utile per semplificare alcune delle dimostrazioni che seguono.

Riportiamo i seguenti risultati da \cite{dovier}, che renderanno più agile l'analisi successiva:
\begin{proposition} \label{prop:rank_bisi_imp_wf}
    Sia $G = (V,E)$. Sia ``\,$\equiv$'' la bisimulazione massima su $G$. Siano $m,n \in WF(G)$. Allora $m \equiv n \implies \rankfunc(m) = \rankfunc(n)$.
\end{proposition}
\begin{proof2}
    Procediamo per induzione su $\rankfunc(m)$. Si ricordi che per il Teorema \ref{theo:bisi_iff_eqsets} gli APG aventi origine in $m,n$ rappresentano lo stesso insieme. Se $\rankfunc(m) = 0, m$ rappresenta l'insieme vuoto. Ma allora lo stesso vale per $n$, per cui anche $\rankfunc(n) = 0$.

    Supponiamo (ipotesi induttiva) che due nodi bisimili di rango minore o uguale a $k-1$ debbano necessariamente avere rango uguale. Sia $\rankfunc(m) = k$, e sia $m' \in V$ tale che $m E m', \rankfunc(m') = k-1$. Allora deve esistere un nodo $n'$ tale che $n E n', m' \equiv n'$, e quindi $\rankfunc(m') = \rankfunc(n')$. Ma allora $\rankfunc(n) \geq k-1 + 1 = k$, per la formulazione alternativa di rank. Analogamente si dimostra che $\rankfunc(m) \geq k$, per cui si ha la tesi.
\end{proof2}
\begin{proposition}
    \label{prop:omega_rank}
    Sia $G = (V,E)$. Sia $m \in V$. Allora:
    \begin{gather*}
        \rankfunc(m) = -\infty \iff \text{l'APG } \restr{G}{m} \text{rappresenta l'insieme } \Omega
    \end{gather*}
\end{proposition}
\begin{proof2}
    Supponiamo che $\rankfunc(m) = - \infty$. Si ricordi la caratterizzazione dell'insieme $\Omega$ (cioè un insieme che contiene solamente se stesso) fornita in \cite{aczel}: un APG rappresenta $\Omega$ se e solo se ogni nodo ha almeno un arco uscente. Dalle ipotesi fatte si deduce che $[m]$ non contiene solamente $m$ (altrimenti $m$ sarebbe un pozzo di $G$). Possiamo dimostrare che ogni nodo in $[m]$ deve avere almeno un arco uscente: infatti, supponendo che non sia così, tale nodo non avrebbe modo di tornare in $[m]$, per cui non potrebbero stare nella stessa SCC.

    Supponiamo ora che l'APG $\restr{G}{m}$ rappresenti l'insieme $\Omega$. Chiaramente $m$ non può essere un pozzo di $G$, altrimenti non rispetterebbe la caratterizzazione. Supponiamo per assurdo che $[m]$ non sia un pozzo di $G^{\superscc}$, cioè da un nodo di $[m]$ parte un arco verso un nodo $x \not\in [m]$. Si presentano quattro possibilità:
    \begin{enumerate}
        \item Da $x$ non esce alcun nodo: impossibile per la caratterizzazione di $\Omega$;
        \item Da $x$ esce un unico nodo che ha per destinazione $x$ stesso: $[x]$ contiene solamente $x$, ed è una foglia di $G^{\superscc}$, per cui ha rango $-\infty$;
        \item Da $x$ esce un nodo che ha per destinazione un nodo di $[m]$: impossibile perchè si è supposto $x \not\in [m]$;
        \item Da $x$ esce un nodo che ha per destinazione un nodo $x' \not\in [m]$.
    \end{enumerate}
    Per cui è evidente che le uniche opzioni valide sono la 2. \hspace{-0.4cm} e la 4. Possiamo dimostrare che il blocco di $G^{\superscc}$ contenente una sequenza di nodi connessi secondo queste due regole è necessariamente un pozzo di $G^{\superscc}$: è possibile aggiungere un numero arbitrario di nodi che le rispettino, ma l'ultimo nodo dovrà avere un \emph{self loop}, oppure tornare in un nodo precedente della sequenza. In entrambi i casi i nodi coinvolti ottengono rango $-\infty$, per cui a cascata (per il terzo caso nella definizione di \rankfunc) i nodi precedenti della sequenza hanno sempre rango $-\infty$.
\end{proof2}
\begin{observation}
    Sia $G = (V,E)$. Siano $m,n \in V$. Allora:
    \begin{gather*}
        \rankfunc(m) = \rankfunc(n) = 0 \implies m \equiv n
    \end{gather*}
\end{observation}
\begin{proof2}
    $m,n$ sono pozzi di $G$, per cui rappresentano l'insieme $\emptyset$. Allora $m,n$ sono bisimili per il Teorema \ref{theo:bisi_iff_eqsets}.
\end{proof2}
\begin{proposition}
    Con le stesse ipotesi della Proposizione \ref{prop:rank_bisi_imp_wf}, supponiamo che $m,n \in V - WF(G)$. Allora $m \equiv n \implies \rankfunc(m) = \rankfunc(n)$.
\end{proposition}
\begin{proof2}
    Sempre per il Teorema \ref{theo:bisi_iff_eqsets}, $m,n$ rappresentano lo stesso insieme. Se $\rankfunc(m) = -\infty$, per la Proposizione \ref{prop:omega_rank} $m$ rappresenta $\Omega$, per cui anche $n$ rappresenta $\Omega$. Ma allora $\rankfunc(n) = -\infty$.

    Se $\rankfunc(m) = h > 0$, per come è stato definita la funzione rank deve esistere un nodo \emph{ben fondato} $m'$ raggiungibile da $m$, non necessariamente in modo diretto, tale che $\rankfunc(m') = h-1$ (il rango aumenta solo in corrispondenza di archi verso nodi \emph{ben fondati}). Poichè $m \equiv n$ deve esistere un nodo \emph{ben fondato} $n'$ raggiungibile da $n$ tale che $m' \equiv n'$. Ma allora ($m',n' \in WF(G)$) $\rankfunc(n') = \rankfunc(m') = h-1$, e quindi $\rankfunc(n) \geq \rankfunc(m) = h$. Analogamente si dimostra che $\rankfunc(m) \geq \rankfunc(n)$, per cui si ha la tesi.
\end{proof2}
\begin{theorem}
    \label{theo:bisi_rank}
    Sia $G = (V,E)$. Sia ``\,$\equiv$'' la bisimulazione massima su $G$. Siano $m,n \in V$. Allora
    \begin{gather*}
        m \equiv n \implies \rankfunc(m) = \rankfunc(n)
    \end{gather*}
\end{theorem}
\begin{proof2}
    Perchè i nodi possano essere bisimili devono essere entrambi \emph{ben fondati} o entrambi \emph{non ben fondati}. La tesi segue dalle proposizioni appena dimostrate.
\end{proof2}

Proponiamo di seguito un algoritmo per la computazione del rango dei nodi di un grafo diretto. Sarebbe ragionevole determinare prima le SCC del grafo (con due visite in profondità su $G$ e $G^{-1}$), per poi effettuare una visita in profondità su $G^{\superscc}$ ed una ulteriore su $G$. Nonostante questo sia il metodo più lineare, è anche poco performante: un'implementazione dell'algoritmo FBA che sfrutta questo procedimento risulta quasi sempre più lenta dell'algoritmo FBA.

Per questo motivo useremo l'implementazione proposta in \cite{dovier}:\\
\begin{algorithm}[H]
    \label{alg:rank}
    \KwData{$G = (V,E)$}
    \caption{Compute-Rank}
    \SetKwProg{Fn}{function}{:}{end}
    \Fn{\textup{dsf-rank-visit}($G = (V,E), n$)}{
        n.color = GRAY\;

        \If{$|E(n)| == 0$}{
            n.rank = 0\;
        } \Else{
            max-rank = $-\infty$\;
            \BlankLine
            \BlankLine
            \ForEach{$v \in E(n)$}{
                \If{v.color == WHITE or v.color == GRAY or !v.wf}{
                    n.wf = false\;
                }
                \BlankLine
                \If{v.color == WHITE}{
                    dfs-rank-visit($G, v$)\;
                }
                \BlankLine
                \If{v.rank != None}{ \label{alg:set_rank}
                    \If{v.wf}{
                        n.rank = max(n.rank, v.rank + 1)\;
                    } \Else{
                        n.rank = max(n.rank, v.rank)\;
                    }
                }
            }
        }

        n.color = BLACK\;
    }
    \Begin{
        DFS($G^{-1}$)\;\label{alg:rank_inverse_dfs}
        \BlankLine
        \BlankLine
        Ordina $V$ in ordine decrescente di \emph{finishing-time}\;
        \ForEach{$v \in V$}{
            Ordina $E(v)$ in ordine decrescente di \emph{finishing-time}\;
        }
        \BlankLine
        \BlankLine
        \ForEach{$n \in V$}{
            n.color = WHITE\;
        }
        \BlankLine
        \BlankLine
        \ForEach{$n \in V$}{
            \If{n.color == WHITE}{
                dfs-rank-visit($G,n$)\;
            }
        }
    }
\end{algorithm}

Valgono i seguenti risultati sull'algoritmo appena proposto:
\begin{observation}
    L'Algoritmo \ref{alg:rank} termina, essendo una DFS. Inoltre, per lo stesso motivo, l'algoritmo ha complessità $O(|V| + |E|)$.
\end{observation}
Nel seguito indicheremo con ``$v$.\emph{rank}'' il rango del nodo $v$ impostato dall'algoritmo, e con ``$\rankfunc(v)$'' il rango corretto. Inoltre useremo la notazione ``$v$.ft'' per indicare il \emph{finishing-time} per la DFS su $G^{-1}$ del nodo $v$.
\begin{theorem}
    \label{theo:rank_correct}
    Dopo l'esecuzione dell'Algoritmo \ref{alg:rank} il rango di ogni nodo di $G$ è corretto, cioè $\forall v \in V\, \rankfunc(v) = v$.\emph{rank}.
\end{theorem}
E' importante ricordare che la DFS di $G$ procede secondo l'ordine indotto dalla DFS di $G^{-1}$.

Proponiamo innanzitutto alcuni risultati preliminari che consentono di semplificare la trattazione. La seguente osservazione è riferita alla seconda DFS nell'Algoritmo \ref{alg:rank}, cioè quella su $G$:
\begin{observation}
    \label{obs:rank_img_scc}
    Se $\langle v,u\rangle \in E$ e $v$.ft $> u$.ft, allora $u \in [v]$.
\end{observation}
\begin{proof2}
    Chiaramente da $v$ è possibile raggiungere $u$ per ipotesi. Consideriamo la DFS su $G^{-1}$: ovviamente si avrà $\langle u, v\rangle \in E^{-1}$. Ma se $v$ fosse stato raggiunto per la prima volta durante la visita in profondità di $u$, avremmo $u$.ft $> v$.ft. Allora $v$ viene raggiunto \emph{prima} di $u$, e l'unica possibilità per cui si possa avere $v$.ft $> u$.ft è che esista un percorso da $v$ a $u$ (in $G^{-1}$). Procedendo al contrario lungo questo cammino si verifica che da $u$ è possibile raggiungere $v$.
\end{proof2}
L'osservazione può essere riformulata in modo più esplitito:
\begin{corollary}
    Se durante la visita dell'immagine $v$ si incontra un vertice $u$ di colore bianco, $v$ è raggiungibile da $u$ e viceversa.
\end{corollary}
E può essere anche generalizzata:
\begin{corollary}
    \label{cor:scc_minore_inglobamento}
    Se $v$.ft $> u$.ft, e $u$ è raggiungibile da $v$, allora $u \in [v]$.
\end{corollary}
Possiamo dedurre alcuni risultati interessanti:
\begin{corollary}
    Se $\langle v,u\rangle \in E$ e $v$.ft $> u$.ft, $u,v \in NWF(G)$. Quindi il criterio usato dall'Algoritmo \ref{alg:rank} per impostare il campo ``\emph{wf}'' dei nodi è corretto.
\end{corollary}
\begin{corollary}
    \label{cor:no_buchi_scc}
    Il \emph{finishing-time} dei nodi di una SCC è un intervallo di interi senza buchi.
\end{corollary}
\begin{proof2}
    Supponiamo per assurdo che esista un nodo $x$ che trasgredisca questa proprietà ($x$.\emph{ft} $= i$). Supponiamo che $v_1, \dots, v_n \in [v]$ (in ordine decrescente di \emph{finishing-time}), e che $v_n$.\emph{ft} $= i-1$. Inoltre supponiamo che ci siano altri nodi in $[v]$, tutti con \emph{finishing-time} minore di $v_n$.\emph{ft}. Chiaramente deve valere $\langle v_n,x \rangle \in E^{-1}$, o equivalentemente $\langle x,v_n \rangle \in E$. Allora $x \in [v]$ per l'Osservazione \ref{obs:rank_img_scc}.
\end{proof2}
Per comodità, nel seguito adotteremo la seguente notazione:
\begin{align*}
    [v].\emph{ft}^* &\coloneqq \max_{x \in [v]} x.\emph{ft}\\
    [v].\emph{ft}_* &\coloneqq \min_{x \in [v]} x.\emph{ft}
\end{align*}
Deduciamo il seguente risultato:
\begin{proposition}
    \label{prop:rank_independent}
    $\rankfunc(v)$ dipende esclusivamente dal rango dei nodi $u$ che soddisfano la seguente proprietà:
    \begin{gather}
        u \in [v] \lor u.ft > [v].ft^* \label{rank_independency_formula}
    \end{gather}
\end{proposition}
\begin{proof2}
    Sia un nodo $x$ per cui non vale nessuna delle due condizioni, quindi $u$.\emph{ft} $< [v].\emph{ft}_*$. Allora dai nodi di $[v]$ non è possibile raggiungere $u$ (nemmeno con cammini più lunghi di un arco): se per assurdo fosse possibile, per il Corollario \ref{cor:scc_minore_inglobamento} si avrebbe $u \in [v]$; allora, per la definizione di \rankfunc, si ha la tesi.
\end{proof2}
Abbiamo spiegato perchè risulta conveniente operare la DFS su $G$ in ordine decrescente di \emph{finishing-time}: ad ogni passo saranno già disponibili le uniche informazioni necessarie, cioè il rango dei nodi con \emph{finishing-time} maggiore.

Possiamo ora dimostrare il Teorema \ref{theo:rank_correct}:
\begin{proof2}
    Sia $v$ un nodo. Se $v$.\emph{rank} $= 0$, $v$ è una foglia.

    Se $v$.\emph{rank} $\neq 0$, poichè il rango viene calcolato come da definizione, è sufficiente dimostrare che al momento della visita di $E(v)$ si ha $u$.\emph{rank} $= \rankfunc(u) \,\forall u \in E(v)$. Chiaramente gli unici nodi problematici sono quelli di colore grigio al momento della visita, in quanto l'esplorazione della loro immagine non è ancora terminata. Questi nodi stanno in $[v]$, e per il Corollario \ref{cor:no_buchi_scc} non ci sono nodi esterni alla SCC nell'intervallo, se si considera il \emph{finishing-time}. Ma allora tutti i nodi il cui rango è rilevante sono già stati visitati (Proposizione \ref{prop:rank_independent}).
\end{proof2}

\subsubsection{L'algoritmo FBA}
Per comodità di notazione poniamo $\mathbb{N}^* \coloneqq \mathbb{N} \cup \{-\infty, 0\}$. Presentiamo lo pseudocodice per l'Algoritmo Dovier-Piazza-Policriti:\\
\begin{algorithm}[H]
    \label{alg:fba}
    \KwData{$G = (V,E)$}
    \caption{Algoritmo FBA}
    \SetKwProg{Fn}{function}{:}{end}
    \Fn{\textup{collapse}($G = (V,E), B \subseteq V$)}{
        Sia $u \in V$ scelto casualmente\;
        \ForEach{$v \in B$}{
            \ForEach{$e \in E \mid$ ``$e$ è incidente a $v$''}{
                \tcp*[h]{$e'$ è l'arco $e$ se si sostuisce $u$ al posto di $v$}\\
                $E = (E - \{e\}) \cup \{e'\}$\;
            }
            $V = V - \{v\}$\;
        }
        \Return{$u$}\;
    }
    \Fn{\textup{Split2}($G = (V,E), P, u, \widehat{B}$)} {
        \ForEach{$B \in P \mid B \in \widehat{B}$}{
            $P = (P - \{B\}) \cup \{\{v \in B \mid v E u\}, \{v \in B \mid \neg (v E u)\}\}$\;
        }
    }
    \Begin{
        Compute-Rank($G$)\;
        $\rho \coloneqq \max\{\rankfunc(n) \mid n \in V\}$\;
        $B_k \coloneqq \{n \in V \mid \rankfunc(n) = k\}, \,\, k \in \mathbb{N}^*$\;
        $P \coloneqq \{B_i \mid i \in \mathbb{N}^*\}$\; \label{alg:partizione_rudimentale}

        \tcp*[h]{$u$ è il nodo di $B_{-\infty}$ preservato da collapse}\\
        $u  \coloneqq$ collapse($G,B_{-\infty}$)\; \label{alg:impunemente_omega}
        Split2($G, P, u, \bigcup_{i=0}^\rho B_i$)\; \label{alg:split_post_omega}

        \ForEach{$i = 0, \dots, \rho$}{
            \label{alg:fba_principal_loop}
            \tcp*[h]{I blocchi di $P$ aventi rango $i$}\\
            $D_i \coloneqq \{B \in P \mid B \subseteq B_i\}$\;
            \tcp*[h]{Il sottografo di $G$ dei nodi di rango $i$}\\
            $G_i = (B_i, (B_i \times B_i) \cap E)$\;
            \tcp*[h]{$D_i$ è una partizione di $G_i$}\\
            $D_i =$ Paige-Tarjan($G_i, D_i$)\;

            \ForEach{$B \in D_i$}{
                $u \coloneqq$ collapse($G,B$)\; \label{alg:collapse_rank}
                Split2($G, P, u, \bigcup_{j=i+1}^\rho B_j$)\;
            }
        }
    }
\end{algorithm}

La funzione ``collapse'' rimuove dal grafo tutti i nodi all'interno di un blocco, ad eccezione di un nodo scelto in modo casuale. Dopodichè sostituisce il nodo mantenuto nel ruolo dei nodi rimossi in ogni arco incidente ad un nodo rimosso. Questa funzione viene usata per contrarre i nodi in un blocco per cui si è già stabilito che siano tutti bisimili. La funzione ``Split2'' è analoga alla funzione ``\splitfunc'', ma consente di specificare i blocchi che possono essere divisi (cioè solo quelli in $\widehat{B}$), e prende in considerazione un unico vertice (cioè $u$) come destinazione degli archi partenti dai blocchi in $\widehat{B}$.

L'algoritmo inizia con il calcolo del rango dei nodi del grafo. Alla Riga \ref{alg:partizione_rudimentale} viene creata una partizione iniziale, da rifinire, i cui blocchi sono composti dai nodi aventi lo stesso rango. Per il Teorema \ref{theo:bisi_rank} sarà sicuramente necessario rifinire questa partizione per determinare la bisimulazione massima: non è possibile infatti che vi siano due nodi bisimili che vengono sistemati in due blocchi differenti a questo punto.

Per la Proposizione \ref{prop:omega_rank} possiamo impunemente considerare bisimili tutti i nodi di rango $-\infty$. Per questo motivo alla Riga \ref{alg:impunemente_omega} viene ``collassato'' il blocco $B_\infty$. Si noti che un'assunzione del genere non è valida per altri valori del rango, in quanto nodi non bisimili possono avere lo stesso rango. A questo punto è necessario aggiornare la partizione e dividere ogni blocco che non rispetta la condizione di stabilità (Riga \ref{alg:split_post_omega}).

In seguito, per ogni rango a partire 0, si considerano i blocchi di rango $i$ e si isola il sottografo contenente solamente nodi composti dai nodi che vi sono contenuti. Si applica l'algoritmo PTA a questo sottografo, in modo da calcolarne la RSCP. I blocchi di questa nuova partizione vengono ``collassati'', ed analogamente a quanto fatto in precedenza si impone ai blocchi di rango superiore la condizione di stabilità.

\subsubsection{Correttezza e complessità}
Presentiamo i seguenti risultati, che costituiscono un analisi completa dell'Algoritmo \ref{alg:fba}, sulla base della discussione proposta in \cite{dovier}:
\begin{theorem}
    Due nodi $m,n$ vengono ``collassati'' in un unico nodo durante l'esecuzione dell'Algoritmo \ref{alg:fba} se e solo se sono bisimili.
\end{theorem}
\begin{proof2}
    Siano $m,n \in V \mid m \equiv n$. Per il Teorema \ref{theo:bisi_rank} $m,n$ devono necessariamente stare all'interno dello stesso blocco nella partizione iniziale creata alla Riga \ref{alg:partizione_rudimentale}. Dimostriamo ora che durante l'esecuzione dell'algoritmo $m,n$ verranno ``collassati'' in un unico nodo.

    Se $\rankfunc(m) = \rankfunc(n) = -\infty$ il blocco contenente questi due nodi viene ``collassato'' alla Riga \ref{alg:impunemente_omega}.

    Per i valori positivi del rango, procediamo per induzione. Se $rank(m) = rank(n) = 0$ inizialmente appartengono entrambi a $B_0$. Sicuramente non vengono divisi da Split2 alla Riga \ref{alg:split_post_omega}, e l'intero blocco $B_0$ viene ``collassato'' alla Riga \ref{alg:collapse_rank}. Se $rank(m) = rank(n) = k > 0$, poichè abbiamo supposto che $m \equiv n$, si ha che $m E m' \implies \exists n' \mid n E n' \land m' E n'$ (e lo stesso vale partendo da $n$). Questo significa che l'esistenza di un arco uscente da uno dei due nodi implica l'esistenza di un altro arco, uscente dall'altro nodo, e che le due destinazioni sono bisimili. Ma allora $m,n$ non possono essere stati divisi nei passaggi precedenti, perchè per l'ipotesi induttiva i nodi bisimili tra loro di rango inferiore a $k$ appartengono allo stesso blocco all'inizio dell'iterazione $i$-esima del ciclo alla Riga \ref{alg:fba_principal_loop}. Non c'è ancora stata alcuna divisione di blocchi indotta da nodi di rango uguale a $k$ (gli unici verso cui un nodo di rango $k$ può avere un arco, oltre ai nodi di rango strettamente minore di $k$), per cui $m,n$ sono necessariamente nello stesso blocco all'inizio dell'iterazione $i$-esima. Per la correttezza dell'Algoritmo PTA, $m,n$ appartengono ancora allo stesso blocco quando questo viene collassato alla Riga \ref{alg:collapse_rank}. In modo analogo si dimostra che se due nodi appartengono allo stesso blocco quando questo viene ``collassato'', allora sono necessariamente bisimili.
\end{proof2}

Prima di valutare la complessità, discutiamo il costo delle funzioni ausiliarie:
\begin{observation}
    Un'implementazione efficiente congiunta delle funzioni ``collapse'' e ``Split2'' ha complessità $\Theta(|E^{-1}(B)|)$, dove $B$ è il blocco su cui sono chiamate in sequenza.
\end{observation}
\begin{proof2}
    \accente sufficiente creare inizialmente l'insieme $E^{-1}(\{v\}) \, \forall v \in V$, cioè la contro-immagine di $v$ rispetto alla relazione binaria $E$. Per ``collassare'' il blocco $B$ e ricalcolare la partizione, cioè per eseguire le due seguenti righe di pseudocodice:
    \begin{align*}
        &u \coloneqq \textup{collapse}(G,B);\\
        &\textup{Split2}(G, P, u, \bigcup_{j=i+1}^\rho B_j);
    \end{align*}
    è sufficiente visitare in modo esaustivo l'insieme $E^{-1}(B)$ e per ogni blocco $C$ della partizione iniziale (tra quelli contenuti in $\bigcup_{j=i+1}^\rho B_j$) raggiunto da questa visita rimuovere da $C$ i nodi che rientrano nella contro-immagine di $B$, ed inserirli in un nuovo blocco.
\end{proof2}
\begin{theorem}
    L'Algoritmo \ref{alg:fba} ha complessità $O(|E| \log |V|)$.
\end{theorem}
\begin{proof2}
    Il rango dei nodi del grafo si può calcolare con l'Algoritmo \ref{alg:rank}, che ha complessità $O(|V|+|E|)$. Per l'osservazione precedente le operazioni alle Righe \ref{alg:impunemente_omega}, \ref{alg:split_post_omega} hanno un'implementazione lineare.

    Il ciclo alla Riga \ref{alg:fba_principal_loop} è composto da tre elementi:
    \begin{enumerate}
        \item La creazione dell'insieme $D_i$ e del sottografo $\restr{G}{i}$;
        \item La chiamata all'Algoritmo PTA;
        \item Il ciclo sui blocchi appartenenti all'insieme $D_i$.
    \end{enumerate}
    Gli elementi di tipo 1 sono chiaramente dominati dagli elementi di tipo 2, in quanto determinare il sottografo $\restr{G}{i}$ ha complessità $O(|B_i| + |E \cap (B_i \times B_i)|)$. La complessità degli elementi di tipo 2 invece è $O(|E \cap (B_i \times B_i)| \log |B_i|)$, come abbiamo già dimostrato.  Per lo stesso motivo, vista l'osservazione precedente, anche gli elementi di tipo 3 sono dominati dagli elementi di tipo 2.

    Ne segue che la complessità dell'algoritmo è:
    \begin{align*}
        T(V,E) &= O(|V| + |E|) + \sum_{i=0}^\rho O(|E \cap (B_i \times B_i)| \log |B_i|)\\
        &= O(|V| + |E|) +  O(\log |V|) \sum_{i=0}^\rho O(|E \cap (B_i \times B_i)|)\\
        &= O(|V| + |E|) +  O(\log |V| |E|)\\
        &= O(|E| \log |V|)
    \end{align*}
\end{proof2}

\newpage
\subsection{L'algoritmo incrementale di Saha}
Finora abbiamo trattato solamente algoritmi che determinano la \rscpnomath (e quindi la bisimulazione massima) a partire dal grafo e da una eventuale partizione iniziale. Se in seguito il grafo viene modificato, ad esempio con l'aggiunta di un arco, per ottenere la nuova \rscpnomath è necessario far ripartire l'algoritmo da zero. L'algoritmo di Saha, presentato in \cite{saha}, consente di ridurre drasticamente la complessità del ricalcolo della \rscpnomath quando sono note la trasformazione del grafo (l'aggiunta di un arco) e la \rscpnomath prima della trasformazione. Come vedremo, l'algoritmo si basa sulla considerazione di diversi casi in cui sono necessarie operazioni differenti. L'ingrediente fondamentale per l'Algoritmo di Saha è, come per Dovier-Piazza-Policriti, la funzione \rankfuncnomath.

Introduciamo innanzitutto l'idea che sta alla base dell'algoritmo oggetto di questa sezione; forniremo poi una serie di risultati che supportano l'intuizione con un apparato formale, ed infine presenteremo l'algoritmo vero e proprio. Infine tratteremo la complessità e la correttezza del procedimento illustrato.

Nel seguito useremo la seguente notazione: aggiungendo un apice singolo dopo un simbolo ($G', X', \dots$) si intende designare l'oggetto matematico rappresentato dal simbolo senza apice, dopo la trasformazione ($G'$ è il grafo $G$ con l'aggiunta del nuovo arco). Inoltre, per mantenere la notazione proposta in \cite{saha}, indicheremo con $X$ la \rscpnomath di $G$, e con $X'$ la \rscpnomath dopo l'aggiunta del nuovo arco. Scriveremo inoltre $A \implies B$ se $A,B$ sono blocchi di una stessa partizione e $\exists a \in A, b \in B \mid \langle a, b\rangle$ è un arco esistente.

L'algoritmo si compone essenzialmente di tre fasi (indicheremo con $X_i$ la partizione ottenuta dopo la fase $i$-esima):
\begin{enumerate}
    \item \emph{Split}, si usa una versione leggermente modificata dell'algoritmo di Paige-Tarjan su $G'$, con partizione iniziale uguale a $X$ (che deve essere nota a priori);
    \item \emph{Merge}, si ricalcola il rango dei nodi di $G'$ e si determina $WF(G')$. In seguito alcuni blocchi di $X_1$ vengono uniti, secondo un criterio che vedremo nel seguito del lavoro;
    \item \emph{Split}, è necessaria solo in alcuni casi: $X_2$ è in generale un'approssimazione più grossolana della \rscpnomath effettiva, ma è possibile individuare alcuni casi in cui questa seconda rifinitura risulta superflua.
\end{enumerate}

\subsubsection{Risultati preliminari}
Al fine di spiegare l'intuizione dell'algoritmo e di semplificare la seguente trattazione della complessità e della correttezza, presentiamo alcuni risultati.

\begin{proposition}
    Siano $u,v \in V'$ due nodi non bisimili di $G'$. Allora $u,v$ appartengono a blocchi diversi di $X_1$.
\end{proposition}
Ricordiamo che $X_1$ è la partizione ottenuta al termine della prima fase di \emph{Split}.
\begin{proof2}
    Segue dalla correttezza dell'Algoritmo di Paige-Tarjan.
\end{proof2}

Non possiamo fermarci ad $X_1$ per ovvi motivi: essa è la \rscpnomath di $G'$ con partizione iniziale $X$, che è una rifinitura della partizione iniziale vera e propria (e che nel seguito chiameremo $Q$). In altre parole, $X_1$ è la \rscpnomath per una partizione iniziale più ``restrittiva'': per questo motivo possiamo sperare che la soluzione effettiva sia in realtà \emph{più grossolana} rispetto ad $X_1$.

A questo punto vengono ricalcolati il rango dei nodi e $WF(G)$. La seconda fase dell'algoritmo consiste nell'unione di alcuni blocchi di $X_1$ sulla base della seguente intuizione: supponiamo di applicare l'Algoritmo di Paige-Tarjan al grafo \emph{prima} dell'aggiunta del nuovo arco (che supponiamo essere $\langle u, v\rangle$). Il blocco $U$ contenente $u$ potrebbe essere stato diviso, ad un certo punto del procedimento, nei blocchi $U_1, U_2$, dove $U_1 \coloneqq U \cap E^{-1}(V), U_2 \coloneqq U - E^{-1}(V), v \in V$, e per la mancanza dell'arco $\langle u,v\rangle, u \in U_2$. La difficoltà della seconda fase sta dunque nel riformare $U$ unendo $U_1, U_2$, e nel determinare poi se sia necessario unire altri blocchi secondo lo stesso criterio conseguentemente all'unione di $U_1,U_2$.

\begin{proposition}
    Siano $u,v \in V'$ due nodi bisimili di $G'$. Allora $u,v$ appartengono allo stesso blocco di $X_2$.
\end{proposition}
\begin{proof2}
    In $X_1$ gli unici nodi bisimili che possono trovarsi in blocchi differenti sono quelli che in $X$ non erano ancora bisimili per la mancanza dell'arco $\langle u,v\rangle$. Riformando $U$ (e a catena gli altri blocchi per i quali almeno un nodo ha un arco con destinazione in $U$) la mancanza viene sanata.
\end{proof2}

Si nota facilmente che in generale la mancanza viene sanata \emph{per eccesso}, ovvero i nuovi blocchi formati nella fase di \emph{Merge} devono essere nuovamente rifiniti per poter ottenere la \rscpnomath.

\begin{observation}
    Il rango dei blocchi uniti nella seconda fase è non decrescente.
\end{observation}
\begin{proof2}
    \accente una conseguenza del fatto che la seconda coppia di blocchi che viene unita ha necessariamente un nodo verso il blocco $U$, la terza coppia ha necessariamente un nodo verso il blocco che risulta dall'unione della seconda coppia di blocchi, e così via. Inoltre $\langle a,b \rangle \in E \implies \rankfunc(a) \geq \rankfunc(b)$, e per la correttezza dell'Algoritmo di Paige-Tarjan i vertici all'interno di uno stesso blocco di $X_1$ hanno tutti lo stesso rango.
\end{proof2}

Proponiamo una condizione necessaria per verificare se due blocchi possono essere uniti.

\begin{definition}
    \label{def:causal_splitter}
    Siano $B,B' \in X_1$. Un blocco $C \in X_1$ è un \emph{causal splitter} di $B_1,B_2$ se valgono le seguenti condizioni:
    \begin{itemize}
        \item $C \in X'$;
        \item $B \implies C \land B' \centernot\implies C$ oppure $B' \implies C \land B \centernot\implies C$.
    \end{itemize}
\end{definition}

Un \emph{causal splitter} di due blocchi $B_1,B_2$ è intuitivamente uno dei blocchi che potrebbe aver determinato la divisione di $B_1,B_2$. Dimostriamo alcune proprietà che verranno tacitamente usate nel seguito.

\begin{observation}
    Siano $B_1, B_2, B_3$ tre blocchi, e sia $C$ un blocco che \emph{non è} un \emph{causal splitter} di $B_1,B_2$ nè di $B_2,B_3$. Allora:
    \begin{itemize}
        \item $C$ non è un \emph{causal splitter} di $B_1,B_3$;
        \item Sia $B_4 \coloneqq B_1 \cup B_2$. $C$ non è un \emph{causal splitter} di $B_3, B_4$;
        \item Siano $B_1, B_2, C_1, C_2$ dei blocchi. Supponiamo che $C_1$ non sia un \emph{causal splitter} di $B_1,B_2$. Allora $C_1 \cup C_2$ è un \emph{causal splitter} di $B_1,B_2$ se e solo se $C_2$ è un \emph{causal splitter} di $B_1,B_2$.
    \end{itemize}
\end{observation}
\begin{proof2}
    I tre punti sono conseguenze elementari della definizione.
\end{proof2}

\begin{observation}
    Siano $B,B' \in X_1$. Se esiste un \emph{causal splitter}, allora i nodi di $B$ e $B'$ non sono bisimili.
\end{observation}

La prima richiesta nella Definizione \ref{def:causal_splitter} è fondamentale: considerare come \emph{causal splitter} un blocco che non siamo sicuri faccia effettivamente parte di $X'$ (che, ricordiamo, è la \emph{RSCP} del grafo dopo l'aggiunta del nuovo arco) significa rinunciare ad unire due blocchi. Se in seguito tale \emph{causal splitter} viene unito ad un altro blocco, la condizione di \emph{causal splitter} potrebbe non essere più valida.

Per questo motivo, nel caso in cui non fossimo sicuri dell'effettiva appartenenza di un blocco ad $X'$ questo verrà escluso dalla valutazione dei possibili \emph{causal splitter}. \accente evidente che questa decisione ci porta ad unire blocchi per cui potrebbe, in linea di principio, esistere un \emph{causal splitter}.

Per ovviare a questa mancanza viene introdotta la terza fase, che rimedia all'approssimazione per eccesso che viene fatta nel rinunciare ad alcuni \emph{causal splitter}.
