\subsection{Algoritmo di Hopcroft}
Esaminiamo innanzitutto un algoritmo risolutivo per una versione semplificata del problema, ovvero la minimizzazione di un'automa a stati finiti \cite{hopcroft}. La risoluzione di questo caso particolare ha fornito diversi spunti importanti per l'ideazione di algoritmi risolutivi più generali che saranno presentati nel seguito.

\newcommand{\automata}{$\mathcal{A} = (\mathcal{S},\mathcal{I},\mathcal{U},\mathcal{F},\delta)$ }
\newcommand{\argsautomata}{$\mathcal{S},\mathcal{I},\mathcal{U},\mathcal{F},\delta$ }

\subsubsection{Nozioni fondamentali}
Innanzitutto definiamo il concetto di \emph{automa}, chiaramente centrale nella descrizione del problema:
\begin{definition}
    Consideriamo i seguenti oggetti matematici:
    \begin{itemize}
        \item Un insieme finito $\mathcal{I}$ detto \emph{insieme degli ingressi};
        \item Un insieme finito $\mathcal{U}$ detto \emph{insieme delle uscite};
        \item Un insieme finito $\mathcal{S}$ detto \emph{insieme degli stati}, ad ogni stato è associata un'unica uscita appartenente ad $U$;
        \item Un insieme $\mathcal{F} \subseteq \mathcal{S}$ detto \emph{insieme degli stati finali};
        \item Una funzione $\delta: \mathcal{S} \times \mathcal{I} \to \mathcal{S}$ detta \emph{funzione di trasferimento}.
    \end{itemize}
    Chiameremo $A = (\mathcal{S},\mathcal{I},\mathcal{U},\delta,\mathcal{F})$ \emph{automa}. Useremo la notazione $Out(x)$ per indicare l'output corrispondente allo stato $x$.
\end{definition}
Possiamo rappresentare un'automa con una tabella degli stati, in cui ad ogni riga corrisponde uno stato, a ogni colonna un ingresso, e ogni cella contiene il nuovo stato quando, nel momento in cui il sistema si trova nello stato corrispondente alla riga, si inserisce l'ingresso corrispondente alla colonna. In alternativa possiamo utilizzare una rappresentazione grafica, in cui ogni stato è descritto da un cerchio contenente il nome dello stato, e le transizioni tra stati sono rappresentate da frecce sulle quali viene specificato l'ingresso che ha innescato la transizione.

\begin{example}
    Nella Tabella \ref{fig:tab_automata} è rappresentato un'automa che cambia stato solamente se l'ingresso è ``1'', e lo stato non è ``$c$''.
    \begin{table}[t]
        \centering
        \begin{tabular}{ c | c c }
            \hline
            & 0 & 1\\
            \hline
            a[0] & a & b \\
            b[0] & b & c \\
            c[1] & c & c \\
            \hline
          \end{tabular}
        \caption{Rappresentazione tabellare di un'automa.}
        \label{fig:tab_automata}
    \end{table}
    \label{exa:automata_tab}
\end{example}

\begin{example}
    Nella Figura \ref{fig:automata} è rappresentato lo stesso automa dell'Esempio \ref{exa:automata_tab}.
    \begin{figure}[t]
        \centering
        \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.5cm,
            scale = 1,transform shape]
            \node[state] (a) {$a[0]$};
            \node[state] (b) [right of=a] {$b[0]$};
            \node[state] (c) [right of=b] {$c[1]$};

            \path (a) edge              node {$1$} (b)
                  (b) edge              node {$1$} (c)
                  (a) edge [loop above]             node {$0$} (a)
                  (b) edge [loop above]             node {$0$} (b)
                  (c) edge [loop above]              node {$0/1$} (c);
        \end{tikzpicture}
        \caption{Rappresentazione grafica di un'automa.}
        \label{fig:automata}
    \end{figure}
\end{example}

In alcuni automi è possibile individuare stati che ``si comportano in modo simile''. Informalmente possiamo dire che due stati $a,b \in \mathcal{S}$ tali che $Out(a) = Out(b)$ si comportano in modo simile quando i due stati in cui ricade il sistema in seguito ad una transizione innescata da uno stesso input $i$ a partire dagli stati $a$ o $b$ (ovvero i due stati $\delta(a,i), \delta(b,i)$) si comportano in modo simile. In Figura \ref{fig:automata_eq} illustriamo un esempio di questa situazione: consideriamo gli stati dell'automa rappresentato. Supponiamo di accorpare gli stati $b,c$ in un unico stato, che chiamiamo $B'$. Se ci interessiamo solamente alla sequenza di output ed all'eventuale raggiungimento di uno stato finale, il nuovo automa risulta indistinguibile dal primo.

\begin{example}
    \begin{figure}[b]
        \centering
        \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.5cm,
            scale = 1,transform shape]
            \node[state] (a) {$A[0]$};
            \node[state] (b) [right of=a] {$B[1]$};
            \node[state] (c) [right of=b] {$C[1]$};

            \path (a) edge              node {$1$} (b)
                  (b) edge [bend right]             node {$1$} (c)
                  (c) edge [bend right]             node {$1$} (b)
                  (a) edge [loop above]             node {$0$} (a)
                  (b) edge [loop above]             node {$0$} (b)
                  (c) edge [loop above]             node {$0$} (c);
        \end{tikzpicture}
        \caption{Automa contenente stati equivalenti.}
        \label{fig:automata_eq}
    \end{figure}
    In questo esempio gli stati $B,C$ sono equivalenti secondo la definizione informale proposta sopra.
\end{example}

Proponiamo una possibile definizione formale di equivalenza tra stati. Nel seguito ne dedurremo una equivalente, che consente di stabilire un parallelo con gli argomenti trattati nella Sezione \ref{sec:base}.
\begin{definition}
    \label{def:equivalent_states}
    Sia $\mathcal{I}^*$ l'insieme di tutte le sequenze di input di lunghezza finita. Sia $\delta^* : \mathcal{S} \times \mathcal{I}^*$ la funzione di transizione ``iterata''. Due stati $x,y \in \mathcal{S}$ sono equivalenti se e soltanto se valgono congiuntamente le seguenti condizioni:
    \begin{enumerate}
        \item $Out(x) = Out(y)$;
        \item $\forall i^* \in \mathcal{I}^*,\,\, \delta^*(x,i^*) \in \mathcal{F} \iff \delta^*(y,i^*) \in \mathcal{F}$.
    \end{enumerate}
    Se le condizioni valgono utilizzermo la notazione ``$x \sim y$''.
\end{definition}

\begin{observation}
    La relazione ``$\sim$'' è una relazione di equivalenza sull'insieme degli stati di un'automa.
\end{observation}

La seguente proposizione sarà utile per formulare il problema con la terminologia esposta nella Sezione \ref{sec:rscp}:
\begin{proposition}
    \label{prop:equivalent_states}
    Due stati $x,y$ sono equivalenti nel senso della definizione \ref{def:equivalent_states} se e solo se
    \begin{gather}\label{hop:alternative_sim}
        \forall i \in \mathcal{I}, \quad \delta(x,i) \sim \delta(y,i)
    \end{gather}
    e $Out(x) = Out(y)$.
\end{proposition}
\begin{proof2}
    Supponiamo che $\exists i \in \mathcal{I} : \delta(x,i) \not\sim \delta(y,i)$. Allora, ad esempio:
    \begin{gather*}
        \exists i^* \in \mathcal{I}^* : \delta^*(\delta(x,i),i^*) = \delta^*(x,ii^*) \in \mathcal{F}, \,\, \delta^*(\delta(y,i),i^*) = \delta^*(y,ii^*) \notin \mathcal{F}
    \end{gather*}
    Quindi, per l'esistenza della stringa $ii^*$ si ha $x \not\sim y$. La dimostrazione è speculare se $\exists i^* \in \mathcal{I}^* : \delta^*(x,ii^*) \notin \mathcal{F}, \delta^*(y,ii^*) \in \mathcal{F}$.

    Ora supponiamo che valga la \ref{hop:alternative_sim}. \accente chiaro che $\forall i^* \in \mathcal{I}^*$ si ha:
    \begin{gather*}
        \delta^*(x,ii^*) \sim \delta^*(y,ii^*) \forall i \in \mathcal{I}
    \end{gather*}
    e quindi $x \sim y$.
\end{proof2}

\begin{proposition}
    La relazione ``$\sim$'' è una bisimulazione sull'insieme $\mathcal{S}$, con la relazione binaria $\displaystyle E \,\,\coloneqq \bigcup_{i \in \mathcal{I}, x \in \mathcal{S}} \{(x,\delta(x,i))\}$.
\end{proposition}
\begin{proof2}
    Supponiamo che $x \sim y$. Sia $\langle x, x'\rangle \in E$, cioè $\exists i \in \mathcal{I} : x' = \delta(x,i)$. Sia $y' = \delta(y,i) \implies \langle y, y'\rangle$. Allora, per la Proposizione \ref{prop:equivalent_states}, $x' \sim y'$. Lo stesso argomento vale in modo speculare.
\end{proof2}

Osserviamo che nella definizione di $E$ si perde un'informazione importante, cioè il fatto che per $i \in \mathcal{I}$ fissato $\delta_i(x) \coloneqq \delta(x,i)$ è una funzione, ovvero l'insieme immagine di ogni $x \in \mathcal{S}$ ha cardinalità 1. L'algoritmo di Hopcroft sfrutta questa ipotesi nel procedimento che consente di migliorare l'algoritmo banale che verrà discusso nel seguito del lavoro.

Possiamo ora definire la \emph{minimizzazione per stati equivalenti} di un'automa a stati finiti:
\begin{definition}\label{def:minim_eq_states}
    Sia \automata un'automa a stati finiti. Chiameremo \emph{minimizzazione per stati equivalenti} l'automa $(\mathfrak{S}, \mathcal{I}, \mathcal{U}, \delta_\mathfrak{S}, \mathcal{F}_\mathfrak{S})$ dove
    \begin{itemize}
        \item $\mathfrak{S} = \mathcal{S} / \sim$;
        \item $\delta_\mathfrak{S} : (\mathfrak{S} \times \mathcal{I}) \to \mathfrak{S} \mid \delta(i, x) = y \implies \delta_\mathfrak{S}(i,[x]_\mathfrak{S}) = [y]_\mathfrak{S}$;
        \item $\mathcal{F}_\mathfrak{S} = \mathcal{F} / \sim$.
    \end{itemize}
\end{definition}
Si rammenta che con la notazione $A / \mathcal{R}$ ci riferiamo all'insieme delle classi di equivalenza indotte da una relazione di equivalenza $\mathcal{R} : A \times A$ su un insieme $A$, e che con la notazione $[a]_\mathfrak{X}$ ci riferiamo al blocco di una partizione $\mathfrak{X}$ in cui viene posto $a$.

Prima di concludere l'esposizione delle nozioni preliminari è necessario dimostrare un risultato interessante che lega il problema della minimizzazzione di un'automa a stati finiti con quanto è stato presentato nella sezione \ref{sec:rscp}. A questo scopo dimostriamo i seguenti lemmi, che consentono di dimostrare tale legame in modo agevole:
\begin{lemma}
    \label{lem:part_stab_stesso_blocco}
    Sia \automata un'automa a stati finiti. Sia $\mathfrak{S}$ una partizione di $\mathcal{S}$ stabile rispetto alla famiglia di funzioni $\delta_i, i \in \mathcal{I}$. Allora $\forall (x,y) \in \mathcal{S} \times \mathcal{S}$ tali che $[x]_{\mathfrak{S}} = [y]_{\mathfrak{S}}$ si ha che $\forall i^* \in \mathcal{I}^*$:
    \begin{gather*}
        [\delta^*(x,i^*)]_{\mathfrak{S}} = [\delta^*(y,i^*)]_{\mathfrak{S}}
    \end{gather*}
\end{lemma}
\begin{proof2}
    Procediamo per induzione su $i^*$. Inizialmente i due stati si trovano nello stesso blocco di $\mathfrak{S}$. Ora supponiamo che dopo l'inserimento dell' $(n-1)$-esimo simbolo di $i^*$ i due stati $x_{n-1}, y_{n-1}$ in cui si trova l'automa appartengano ancora allo stesso blocco. La partizione è stabile rispetto alla funzione $\delta_i$, dove $i$ è l' $n$-esimo simbolo di $i^*$, quindi tutto il blocco $[x_{n-1}]_{\mathfrak{S}} = [y_{n-1}]_{\mathfrak{S}}\,\,$ è contenuto all'interno dell'insieme $\delta_i^{-1}([\delta_i(x_{n-1})]_{\mathfrak{S}}) = \delta_i^{-1}([x_n]_{\mathfrak{S}})$. Quindi $\delta_i(y_{n-1}) \in \delta_i^{-1}([x_n]_{\mathfrak{S}})$, e dunque e si ha anche $[x_n]_{\widehat{S}} = [y_n]_{\mathfrak{S}}$.
\end{proof2}

\begin{lemma}
    \label{lem:part_stab_equiv}
    Sia \automata un'automa a stati finiti. Sia $\mathfrak{S}$ una partizione di $\mathcal{S}$ stabile rispetto alla famiglia di funzioni $\delta_i, i \in \mathcal{I}$. Supponiamo inoltre che per $\mathfrak{S}$ valga la segente condizione:
    \begin{gather*}
        \forall (x,F) \in \mathcal{S} \times \mathcal{F}, \quad [x]_{\mathfrak{S}} = [F]_{\mathfrak{S}} \implies x \in \mathcal{F}
    \end{gather*}
    Allora $\forall \mathfrak{p} \in \mathfrak{S}, \,\,\forall (x,y) \in \mathfrak{p} \times \mathfrak{p}$ si ha $x \sim y$.
\end{lemma}
\begin{proof2}
    Supponiamo per assurdo che in un blocco di $\mathfrak{S}$ esistano due stati $x,y$ non equivalenti. Allora deve esistere una stringa $i^* \in \mathcal{I}^*$ tale che, ad esempio, $\delta^*(x,i^*) \in \mathcal{F} \land \delta^*(y,i^*) \not\in \mathcal{F}$. Ma per il Lemma \ref{lem:part_stab_stesso_blocco} $[\delta^*(x,i^*)]_{\mathfrak{S}} = [\delta^*(y,i^*)]_{\mathfrak{S}}$. Chiaramente questo è assurdo per la condizione assunta nell'ipotesi, quindi non possono esistere nello stesso blocco due stati non equivalenti.
\end{proof2}
A questo punto abbiamo gli ingredienti necessari per dimostrare un risultato interessante su una tipologia particolare di automi così definita:
\begin{definition}
    \label{def:automi_ben_posti}
    Un'automa a stati finiti \automata è \emph{ben posto} se $x \in \mathcal{F}, y \not\in \mathcal{F} \implies Out(x) \neq Out(y)$, ovvero l'output assegnato a stati finali è sempre diverso dall'output corrispondente a stati non finali.
\end{definition}
\begin{theorem}
    \label{theo:auto_mini_rscp}
    Sia \automata un'automa a stati finiti. La minimizzazione per stati equivalenti $(\mathfrak{S}, \mathcal{I}, \mathcal{U}, \delta_\mathfrak{S}, \mathcal{F}_\mathfrak{S})$ (Definizione \ref{def:minim_eq_states}) è l'automa avente per stati i blocchi della partizione più grossolana stabile rispetto alla famiglia di funzioni $\delta_i, i \in \mathcal{I}$.
\end{theorem}
\begin{proof2}
    Dimostriamo che $\mathfrak{S}$ è una partizione di $\mathcal{S}$ stabile rispetto alle funzioni $\delta_i$. Supponiamo per assurdo che $\exists i \in \mathcal{I} \mid \mathfrak{S}$ non è stabile rispetto a $\delta_i$. Quindi esistono $\mathfrak{s}_1, \mathfrak{s}_2 \in \mathfrak{S}$ tali che:
    \begin{gather*}
        \mathfrak{s}_1 \not\subseteq \delta_i^{-1}(\mathfrak{s}_2) \,\, \land \,\, \mathfrak{s}_1 \,\cap \,\delta_i^{-1}(\mathfrak{s}_2) \neq \emptyset
    \end{gather*}
    La prima porzione dell'espressione implica che $\exists x \in \mathfrak{s}_1 : \delta_i(x) \not\in \mathfrak{s}_2$. Ma questo è chiaramente in contrasto con l'Osservazione \ref{prop:equivalent_states}, perchè $\forall (y,Z) \in \mathfrak{s}_1 \times \mathfrak{s}_1$ deve valere $\delta_i(y) \sim \delta_i(Z)$, mentre è evidente che, poichè $\mathfrak{s}_1 \,\cap\, \delta_i^{-1}(\mathfrak{s}_2) \neq \emptyset$, c'è almeno una coppia che non soddisfa questa condizione.

    Ora supponiamo che la partizione $\mathfrak{S}$ non sia la più grossolana stabile rispetto a tutte le funzioni $\delta_i : i \in \mathcal{I}$. Ne deve esistere quindi una stabile più grossolana, che chiamiamo $\mathfrak{S}_2$. Chiaramente si ha $|\mathfrak{S}_2| < |\mathfrak{S}|$, e quindi devono esistere almeno due blocchi $\mathfrak{s}_1, \mathfrak{s}_2 \in \mathfrak{S}$ ed un blocco $\mathfrak{s}_3 \in \mathfrak{S}_2$ tali che
    \begin{gather*}
        \mathfrak{s}_1 \cap \mathfrak{s}_3 \neq \emptyset \,\,\land\,\, \mathfrak{s}_2 \cap \mathfrak{s}_3 \neq \emptyset
    \end{gather*}
    Ma questo è chiaramente in contrasto con il Lemma \ref{lem:part_stab_equiv} per come è stata costruita la partizione $\mathfrak{S}$, in quanto non possono esistere coppie di stati nello stesso blocco di $\mathcal{S}_2$ ma in un blocco diverso di $\mathcal{S}$, in quanto da quest'ultima condizione segue che $x \not\sim y$.
\end{proof2}
Osserviamo che la condizione richiesta nell'ipotesi del Teorema \ref{theo:auto_mini_rscp}, che impone l'assegnazione di un output diverso a stati finali e non finali, è estremamente debole: dato un'automa qualsiasi è sempre possibile costruirne un altro che soddisfa tale condizione, in tempo lineare rispetto al numero di stati.

\subsubsection{L'algoritmo di Hopcroft}
Il problema della minimizzazione di automi a stati finiti è stato risolto da John Hopcroft nel 1971, che ha proposto un algoritmo efficiente sfruttando un'osservazione che sarà discussa nel seguito. Presenteremo innanzitutto un algoritmo immediato, che sarà utile per un primo approccio al lato algoritmico del problema; dopodichè esamineremo lo pseudocodice dell'algoritmo di Hopcroft, e commenteremo l'intuizione che ha consentito di migliorare l'algoritmo immediato.

\paragraph{Un primo algoritmo triviale} Utilizzando i risultati della sezione precedente possiamo progettare il seguente algoritmo, il cui funzionamento è sostanzialmente lineare: per ogni ingresso $i \in \mathcal{I}$ si rifinisce la partizione iniziale (data dagli output assegnati ai vari stati) finchè non diventa stabile rispetto a $\delta_i$; dopodichè si procede con l'ingresso seguente.

\begin{algorithm}[t]
    \caption{Algoritmo triviale per la minimizzazzione di automi a stati finiti.}
    \label{alg:hop_banale}
    \SetAlgoLined
    \KwData{\argsautomata}
    \tcp*[h]{Partizione iniziale contenente un unico blocco}
    \nl$\widetilde{S} \coloneqq \{\{s_1, \dots, s_n\}\}$\;
    \tcc*[h]{Separiamo gli stati non equivalenti in base all'output}\\
    $\mathfrak{S} = \{\mathfrak{s}_k \in \mathcal{P}(\mathcal{S}) \mid Out(x) = k, \forall x \in \mathfrak{s}_k\}$\;\label{alg:hop_banale_partiziona_output}
    \ForEach{$i \in I$} {
        \label{alg:hop_banale_for_ingressi}
        \While{($\mathfrak{s}_1, \mathfrak{s}_2 =$ BlocchiNonStabili($\mathfrak{S}, \delta_i)) \neq$ None}{
            \label{alg:hop_banale_for_ns}
            \tcc*[h]{Estraiamo una coppia di blocchi che trasgredisce la condizione di stabilità.}\\
            $\widetilde{\mathfrak{s}}_1 \coloneqq \mathfrak{s}_1 - \delta_i^{-1}(\mathfrak{s}_2)$\;
            $\widehat{\mathfrak{s}}_1 \coloneqq \mathfrak{s}_1 \cap \delta_i^{-1}(\mathfrak{s}_2)$\;
            \tcp*[h]{Aggiorniamo $S$}\\
            $\mathfrak{S} = (\mathfrak{S} - \{\mathfrak{s}_1\}) \cup \{\widetilde{\mathfrak{s}}_1,\widehat{\mathfrak{s}}_1\}$\;
        }
    }
    \Return{$S$}
\end{algorithm}
La funzione ``BlocchiNonStabili'' restituisce una coppia $\mathfrak{s}_1, \mathfrak{s}_2$ di blocchi di $\mathfrak{S}$ che trasgredisce la condizione di stabilità rispetto alla funzione $\delta_i$, o ``None'' quando una tale coppia non esiste. L'algoritmo termina, perchè la condizione del ciclo diventa sicuramente falsa quando la partizione $\mathfrak{S}$ è composta da $n$ blocchi, uno per ogni stato, ed ad ogni iterazione un blocco viene diviso in due blocchi distinti e non vuoti.

Inoltre la risposta fornita è corretta, perchè l'algoritmo si ferma appena viene trovata una partizione stabile. Poichè ad ogni iterazione del ciclo definito nella Riga \ref{alg:hop_banale_for_ns} il numero di blocchi aumenta di 1, il risultato è la partizione stabile più grossolana rispetto alle funzioni $\delta_i$. Osserviamo inoltre che, se la partizione è stabile rispetto a $\delta_i$ dopo una certa iterazione del ciclo definito nella Riga \ref{alg:hop_banale_for_ingressi}, allora resta stabile fino alla fine dell'esecuzione per la Proposizione \ref{prop:rifi_stabile}.

Verifichiamo la complessità dell'algoritmo:
\begin{itemize}
    \item La Riga \ref{alg:hop_banale_partiziona_output} ha complessità $\Theta(|S|)$;
    \item Il ciclo alla Riga \ref{alg:hop_banale_for_ingressi} viene eseguito $\Theta(|I|)$ volte;
    \item La funzione ``BlocchiNonStabili'' ha complessità $O(|S|^2)$;
    \item Il ciclo alla Riga \ref{alg:hop_banale_for_ns} viene eseguito $O(|S|)$ volte;
    \item Il contenuto del ciclo alla Riga \ref{alg:hop_banale_for_ns} ha complessità $O(|S|)$ (con gli opportuni accorgimenti).
\end{itemize}
Quindi la complessità dell'algoritmo è \begin{gather*}
    T(|S|,|I|) = \Theta|I|\left[O(|S|^2 + O(|S|) * O(|S|))\right] = \Theta(I)O(|S|^2)
\end{gather*}
Se consideriamo costante il numero di ingressi: $T(|S|) = O(|S|^2)$. Seppur di facile comprensione, l'algoritmo triviale è poco efficiente a causa dei numerosi controlli per la stabilità.

\paragraph{Algoritmo di Hopcroft} Presentiamo un algoritmo che migliora la procedura triviale presentata nella sezione precedente, portando la complessità relativa alla risoluzione del problema della minimizzazione di automi a stati finiti a \emph{loglineare} \cite{hopcroft}. Forniremo un commento allo pseudocodice, in modo da spiegare il procedimento in modo dettagliato. In seguito analizzeremo formalmente l'algoritmo, proporremo e commenteremo la dimostrazione della correttezza e della complessità.\\
\afterpage{
    \begin{algorithm}[H]
        \thispagestyle{empty}
        \caption{Algoritmo di Hopcroft}
        \label{alg:hop}
        \KwData{\argsautomata}
        \Begin{
            \ForEach{$(s,i) \in \mathcal{S} \times \mathcal{I}$}{
                $\delta^{-1}(s,i) \coloneqq \{t : \delta(t,i) = s\}$\;\label{alg:hop_deltainverso}
            }
            $B(1) \coloneqq \mathcal{F}, B(2) \coloneqq \mathcal{S} - \mathcal{F}$\;
            \ForEach{$j \in \{1,2\}$}{
                \tcc*[h]{$\forall i \in \mathcal{I}$ costruiamo l'insieme degli stati in $B(j)$ aventi controimmagine non vuota rispetto  a $\delta_i$}.\\
                \ForEach{$i \in \mathcal{I}$}{
                    $i(j) = \{s : s \in B(j) \land \delta^{-1}(s,i) \neq \emptyset\}$\;\label{alg:hop_stati_delta_nonvuoto}
                }
            }
            \tcc*[h]{$k$ è il numero di blocchi della partizione, aumenta in seguito alle rifiniture.}\\
            $k \coloneqq 2$\;
            \tcc*[h]{Per ogni ingresso $i$ creiamo un insieme $L(i)$ contenente l'indice $j$ che minimizza $|i(j)|$.}\\
            \ForEach{$i \in \mathcal{I}$}{
                \label{alg:hop_splitters}
                \eIf{$|i(1)| \leq |i(2)|$}{
                    $L(i) = \{1\}$\;
                }{
                    $L(i) = \{2\}$\;
                }
            }
            \While{$\exists i \in \mathcal{I} : L(i) \neq \emptyset$}{
                \label{alg:hop_ciclo_infinito}
                Seleziona un $j \in L(i)$. $L(i) = L(i) - \{j\}$\;
                \tcc*[h]{Per ogni blocco $B(m)$ per cui il procedimento ha senso, usiamo $i(j)$ come \emph{splitter}.}\\
                \ForEach{$m \leq k : \exists t \in B(m) : \delta(t,i) \in i(j)$}{\label{alg:hop_ciclo_blocchi_preimmagine}
                    $B^{'}(m) \coloneqq \{u \in B(m) : \delta(u,i) \in i(j)\}$\;\label{alg:hop_split}
                    $B^{''}(m) \coloneqq B(m) - B^{'}(m)$\;
                    $B(m) = B^{'}(m)$, $B(k+1) \coloneqq B^{''}(m)$\;
                    \ForEach{$a \in I$}{\label{alg:hop_aggiorna_dopo_split}
                        $a(m) = \{s : s \in B(m) \land \delta^{-1}(s,a) \neq \emptyset\}$\;
                        $a(k+1) \coloneqq \{s : s \in B(k+1) \land \delta^{-1}(s,a) \neq \emptyset\}$\;
                        \eIf{$|a(m)| \leq |a(k+1)|$}{
                            $L(a) = L(a) \cup \{m\}$\;
                        }{
                            $L(a) = L(a) \cup \{k+1\}$\;
                        }
                    }
                    $k = k + 1$\;
                }
            }
        }
        \thispagestyle{empty}
    \end{algorithm}
    \clearpage
}
Nella Riga \ref{alg:hop_deltainverso} definiamo $\delta^{-1}$, che consente l'accesso in tempo costante all'insieme degli stati che conducono ad un determinato stato conseguentemente ad un determinato ingresso.\\
Inizialmente la partizione contiene due blocchi: gli stati finali e quelli non finali. Di conseguenza le rifiniture successive della partizione manterranno sempre separati stati finali e non finali.\\
Per evitare accessi inutili (che incrementerebbero i termini costanti dell'algoritmo) nella Riga \ref{alg:hop_stati_delta_nonvuoto} definiamo $i(j)$ per $i \in I, j \in \{1,2\}$ come l'insieme degli stati $s$ nel blocco $B(j)$ per cui esiste qualche stato $t$ tale che $\delta(t,i) = s$.\\
All'interno del ciclo della Riga \ref{alg:hop_splitters} definiamo gli insiemi $L(i)\,\, \forall i \in I$, che contengono gli indici dei blocchi che verranno usati come \emph{splitter} nel seguito del procedimento. Osserviamo che all'interno di questo insieme vengono inseriti gli indici dei blocchi per cui la cardinalità di ``$i(\cdot)$'' è minima. Come dimosteremo nel seguito, questa tecnica

In \cite{paigetarjan} si dimostra che, limitatamente al caso particolare a cui si applica l'algoritmo di Hopcroft, è possibile scegliere gli \emph{splitter} in modo vantaggioso, allo scopo di ridurre il carico di lavoro e dunque la complessità. Riportiamo e commentiamo la dimostrazione:
\begin{proposition}
    Sia $S$ un insieme finito. Sia $f : S E S$ una funzione (cioè $\forall s \in S\,\, |f({s})| = 1$). Sia $\widetilde{S}$ una partizione di $S$. Sia $Q$ l'unione di alcuni blocchi di $\widetilde{S}$, e sia $B$ un blocco di $\widetilde{S}$, con $B \subseteq Q$. Supponiamo $\widetilde{S}$ stabile rispetto a $Q$. Allora
    \begin{center}
        Split$(B,\widetilde{S})$ = Split$(Q - B, $Split$(B,\widetilde{S}))$
    \end{center}
\end{proposition}
\begin{proof2}
    Vogliamo dimostrare che $Q - B$ non è uno \emph{splitter} di Split$(B,\widetilde{S})$, cioè che $\forall S_x \in $ Split$(B,\widetilde{S})$ si ha
    \begin{gather*}
        S_x \subseteq f^{-1}(Q - B) \,\,\lor\,\, S_s \cap f^{-1}(Q-B) = \emptyset
    \end{gather*}
    Ricordando che $\widetilde{S}$ è stabile rispetto a $Q$, $\widehat{S} \coloneqq$ Split$(B,\widetilde{S})$ è stabile rispetto a $Q$ e a $B$, per l'Osservazione \ref{prop:split_eredita}. Di conseguenza per ogni blocco $S_x \in \widehat{S}$ si ha
    \begin{gather*}
        S_x \subseteq f^{-1}(B) \lor S_x \cap f^{-1}(B) = \emptyset\\
        \land\\
        S_x \subseteq f^{-1}(Q) \lor S_x \cap f^{-1}(Q) = \emptyset
    \end{gather*}
    Se $S_x \subseteq f^{-1}(B)$ chiaramente $S_x \,\,\cap \subseteq f^{-1}(Q-B) = \emptyset$. Se $S_x \cap f^{-1}(Q) = \emptyset$, allora $S_x \cap f^{-1}(Q-B) = \emptyset$. Se $S_x \cap f^{-1}(B) = \emptyset \land S_x \subseteq f^{-1}(Q)$ bisogna avere $S_x \subseteq f^{-1}(Q-B)$.\\
    Osserviamo che queste deduzioni valgono solamente se $f$ è una funzione.
\end{proof2}

Dal punto di vista computazionale è chiaramente più conveniente usare come \emph{splitter} l'insieme tra $B$ e $Q - B$ avente cardinalità minore. La strategia di Hopcroft (``\emph{process the smaller half}'', come viene sintetizzata in \cite{paigetarjan}) consiste infatti nella selezione degli \emph{splitter} secondo il criterio della cardinalità, a differenza di quanto avviene nell'Algoritmo \ref{alg:hop_banale}.\\
Dalla Riga \ref{alg:hop_split} alla Riga \ref{alg:hop_aggiorna_dopo_split} viene operato lo ``Split''. Nel ciclo della Riga \ref{alg:hop_aggiorna_dopo_split} vengono aggiornate le strutture dati relative ai nuovi blocchi creati. Osserviamo che ad ogni iterazione del ciclo della Riga \ref{alg:hop_aggiorna_dopo_split} vengono creati due nuovi blocchi, anche se il numero di blocchi aumenta soltanto di 1, perchè viene anche ``smembrato'' un blocco già esistente. Di questi due blocchi se ne sceglie uno da usare come \emph{splitter}, seguendo il criterio illustrato sopra.\\
Queste operazioni vengono ripetute finchè in $L(i)$ per qualche $i$ resta un blocco da usare come \emph{splitter}.
\\\\
Analizziamo la correttezza dell'algoritmo. Trascurando la parte iniziale, ad ogni iterazione del ciclo della Riga \ref{alg:hop_ciclo_infinito} viene prima rimosso un elemento in $L(i)$, e solamente in seguito ad una rifinitura della partizione ne viene inserito un altro. Poichè non è possibile rifinire all'infinito, l'algoritmo termina.\\
Il seguente Teorema dimostra la validità del risultato:
\begin{theorem}
    \label{teo:hop_corretto}
    Sia $A = (S,I,\delta,F)$ un'automa. Sia $\widetilde{S}$ la partizione risultante dall'applicazione dell'Algoritmo \ref{alg:hop}. Siano $x,y \in S$. Allora
    \begin{gather*}
        x \sim y \iff [x]_{\widetilde{S}} = [y]_{\widetilde{S}}
    \end{gather*}
\end{theorem}
\begin{proof2}
    Dimostriamo innanzitutto che $[x]_{\widetilde{S}} \neq [y]_{\widetilde{S}} \implies x \not\sim y$. Procediamo per induzione:
    \begin{itemize}
        \item La relazione vale prima del ciclo \ref{alg:hop_ciclo_infinito}, infatti stati finali e non finali non possono essere equivalenti;
        \item Supponiamo che sia vero prima di una certa iterazione. Supponiamo che gli stati $x,y$ finiscano in partizioni diverse nelle Righe tra \ref{alg:hop_split} e \ref{alg:hop_aggiorna_dopo_split}. Questo significa che $[\delta(x,i)]_{\widetilde{S}_n} \neq [\delta(y,i)]_{\widetilde{S}_n}$, dove $\widetilde{S}_n$ è la partizione costruita dall'algoritmo fino all'iterazione considerata. Ma allora, per l'ipotesi induttiva, $\delta(x,i) \not\sim \delta(y,i)$, e quindi $x,y$ non sono equivalenti (per l'Osservazione \ref{prop:equivalent_states}).
    \end{itemize}
    Questo ragionamento è valido perchè il fatto che due stati si trovino in partizioni differenti dopo una certa iterazione implica che si troveranno in partizioni differenti anche al termine del procedimento.\\
    Dimostriamo ora che $[x]_{\widetilde{S}} = [y]_{\widetilde{S}} \implies x \sim y$. Supponiamo per assurdo che per due stati $x,y$ si abbia $[x]_{\widetilde{S}} = [y]_{\widetilde{S}} \land x \not\sim y$. Allora, ad esempio, $\exists i^* \in I^* : \delta^*(x,i^*) \in F, \delta^*(y,i^*) \not\in F$. Ma poichè inizialmente poniamo stati inziali e finali in partizioni diverse, deve valere chiaramente $[\delta^*(x,i^*)]_{\widetilde{S}} \neq [\delta^*(y,i^*)]_{\widetilde{S}}$, e quindi procedendo a ritroso sui simboli di $i^*$ si ha
    \begin{gather*}
        [\delta^*(x,i^*_n)]_{\widetilde{S}} \neq [\delta^*(y,i^*)_n]_{\widetilde{S}} \implies [\delta^*(x,i^*_{n-1})]_{\widetilde{S}} \neq [\delta^*(y,i^*)_{n-1}]_{\widetilde{S}}
    \end{gather*}
    Per cui si ha chiaramente $[x]_{\widetilde{S}} = [\delta^*(x,i^*_0)]_{\widetilde{S}} \neq [\delta^*(y,i^*)_0]_{\widetilde{S}} = [y]_{\widetilde{S}}$.
\end{proof2}

Discutiamo ora la complessità dell'algoritmo. In questo paragrafo non tratteremo i dettagli dell'implementazione, facilmente reperibili in \cite{hopcroft}, e ci concentreremo unicamente sul contributo del procedimento.\\
Per costruire $\delta^{-1}$ è sufficiente valutare una sola volta ogni stato per ogni ingresso, quindi l'operazione è $\Theta(|S||I|)$. La costruzione delle due partizioni iniziali è $\Theta(|S|)$. La costruzione di $L(i) \,\,\forall i \in I$ è chiaramente $\Theta(|I|)$. Di conseguenza la complessità delle istruzioni precedenti alla Riga \ref{alg:hop_ciclo_infinito} è $\Theta(|S||I|)$.\\
Prima di procedere, dimostriamo il seguente risultato:
\begin{proposition}
    \label{obs:log}
    Sia
    \begin{gather*}
        f_a(x) \coloneqq a\log_2(a) - (a-x)\log_2(a-x) - x\log_2(x) \qquad a > 0,\,\, 0 < x < a
    \end{gather*}
    Tale funzione ha massimo in $\frac{a}{2}$, con $f_a(\frac{a}{2}) = a$, ed è strettamente positiva sul dominio considerato.
\end{proposition}
\begin{proof2}
    L'osservazione si dimostra con un semplice studio di funzione.
\end{proof2}
Il seguente risultato fornisce un \emph{upper-bound} per il contributo al tempo di esecuzione di tutte le iterazioni del ciclo in cui si seleziona l'ingresso $i$ considerato sopra, dalla $n$-esima fino alla terminazione dell'algoritmo:
\begin{theorem}
    Sia $i$ un ingresso. Consideriamo l'iterazione $n$-esima del ciclo della Riga \ref{alg:hop_ciclo_infinito} per un $n$ qualsiasi per cui si abbia la seguente configurazione:
    \begin{gather*}
        \widetilde{S}_n = \{S_1,\dots,S_m\}, \quad L(i) = \{i_1,\dots,i_r\}, \quad  \{i_{r+1},\dots,i_m\} = \{1,\dots,m\} - L(i)
    \end{gather*}
    Allora il contributo alla complessità dato da tutte le iterazioni in cui si seleziona l'ingresso $i$, dalla $n$-esima fino alla terminazione dell'algoritmo, è maggiorato dalla seguente espressione:
    \begin{gather*}
        T_i^n = k\left(\sum_{j = 1}^r |i(i_j)|\log_2|i(i_j)| + \sum_{j = r+1}^m |i(i_j)|\log_2\frac{|i(i_j)|}{2}\right)
    \end{gather*}
\end{theorem}
\begin{proof2}
    Procediamo per induzione ``al contrario''. Chiaramente la maggiorazione è valida alla terminazione dell'algoritmo, ovvero dopo l'ultima iterazione. Ora supponiamo che la maggiorazione sia valida dopo l'iterazione $k$-esima (con $k > n$), e dimostriamo che questo implica la validità della stessa prima dell'iterazione $k$-esima, ovvero al termine dell'iterazione $(k-1)$-esima. In altre parole è necessario dimostrare che la somma tra $T_i^{k-1}$ ed il tempo impiegato per l'esecuzione dell'iterazione $k$-esima è minore o uguale di $T_i^k$. La seguente osservazione consente di quantificare il contributo di una singola iterazione:
    \begin{observation*}
        Se all'inizio dell'iterazione viene selezionato l'indice $x \in L(i)$, la complessità dell'iterazione è $O(|i(x)|)$, cioè il tempo impiegato durante l'esecuzione è maggiorato da $k|i(x)|$, dove $k$ è una costante di propozionalità. Possiamo trarre questa conclusione perchè il ciclo della Riga \ref{alg:hop_ciclo_blocchi_preimmagine} è in realtà un ciclo sugli stati $s\in i(x)$, che per ognuno estrae (in tempo costante) $\delta^{-1}(s,i)$.
    \end{observation*}
    All'inizio di ogni iterazione viene estratto un ingresso $a$. Si presentano due casi:
    \begin{itemize}
        \item $a \neq i$: poichè $T_i^k$ prende in considerazione solamente il tempo impiegato dalle iterazioni in cui viene selezionato $i$, questa iterazione è esclusa dalla stima. Ciononostante l'iterazione può modificare i blocchi della partizione $\widetilde{S}_k$, e dobbiamo quindi verificare che $T_i^{k-1} \leq T_i^k$:
        \begin{itemize}
            \item Se viene modificato un blocco $B(x)$ con $x \in L(i)$, nella stima dobbiamo sostituire un elemento maggiorabile con $b \log_2 b$ con un elemento del tipo $c \log_2 c + (b-c) \log_2 (b-c)$. Per l'osservazione \ref{obs:log} si ha $T_i^{k-1} < T_i^k$;
            \item Se invece $x \not\in L(i)$, in $T_i^{k-1}$ dobbiamo sostituire un elemento del tipo $b \log_2 \frac{b}{2}$ con un elemento del tipo $c \log_2 c + (b-c) \log_2 \frac{b-c}{2}$ (supponendo che $c \leq b - c$, in caso contrario la dimostrazione è simile). Infatti il blocco avente cardinalità $b-c$ fa parte, alla fine dell'iterazione, dell'insieme dei blocchi il cui indice non appartiene ad $L(i)$, ed al termine del ciclo della Riga \ref{alg:hop_aggiorna_dopo_split} si ha $c \in L(i)$. Chiaramente si ha $c \leq \frac{b}{2}$, e quindi:
            \begin{align*}
                c \log_2 c + (b-c) \log_2 \frac{b-c}{2} &\leq c \log_2 \frac{b}{2} + (b-c) \log_2 \frac{b}{2}\\
                &= (c + b - c) \log_2 \frac{b}{2}\\
                &= b \log_2 \frac{b}{2}.
            \end{align*}
        \end{itemize}
        \item $a = i$: come abbiamo osservato sopra, il tempo impiegato all'interno dell'iterazione è $O(|i(x)|)$, dove $x$ è l'indice estratto da $L(i)$. Poichè si ha:
        \begin{align*}
            T_i^{k-1} + k|i(x)| = k \Bigg( |i(x)| + |i(x)|\log_2 \frac{|i(x)|}{2} &+ \sum_{\substack{j = 1\\i_j \neq x}}^r |i(i_j)|\log_2|i(i_j)| \\
            &+ \sum_{j = r+1}^m |i(i_j)|\log_2\frac{|i(i_j)|}{2}\Bigg)
         \end{align*}
        dove consideriamo il tempo impiegato per l'iterazione, ed il fatto che l'indice $x$, al termine dell'iterazione, fa parte dell'insieme dei blocchi il cui indice non appartiene ad $L(i)$. Poichè vale:
        \begin{align*}
            |i(x)| + |i(x)|\log_2 \frac{|i(x)|}{2} &= |i(x)| + |i(x)|\log_2 |i(x)| - |i(x)|\log_2 2\\
            &= |i(x)| + |i(x)|\log_2 |i(x)| - |i(x)|\\
            &= |i(x)|\log_2 |i(x)|
        \end{align*}
        si deduce facilmente la seguente relazione:
        \begin{gather*}
            T_i^{k-1} + k|i(x)| = T_i^k.
        \end{gather*}
    \end{itemize}
    \vspace*{-0.8cm}
\end{proof2}
Prima della prima iterazione del ciclo della Riga \ref{alg:hop_ciclo_infinito}, per un $i \in I$ fissato, (supponendo $|S-F| \geq |F|$) si ha
\begin{gather*}
    T_i = k\left(|S-F|\log_2|S-F| + |F|\log_2\frac{|F|}{2}\right)
\end{gather*}
che, per l'Osservazione \ref{obs:log}, si maggiora con $k |S|\log_2|S|$. Allora la complessità dell'algoritmo è data dalla somma della complessità delle righe precedenti alla \ref{alg:hop_ciclo_infinito}, ovvero $\Theta(|I||S|)$, e $|I| * O(|S|\log|S|)$, cioè $|I||S|\log|S|$. Considerando costante la cardinalità di $I$:
\begin{gather*}
    T_{alg} = O(|S|\log|S|).
\end{gather*}
